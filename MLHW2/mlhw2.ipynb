{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63293144",
   "metadata": {},
   "source": [
    "# 准备和测试模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39e5dcbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9430456b4984e229d9ef4aef3da81e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from modelscope import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_path = \"/home/yxlin/huggingface/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8573949d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(_model, _tokenizer, _messages: str) -> str:\n",
    "    ''' Use Ai generate respone base on you question\n",
    "    =====Params=====\n",
    "    _messages format\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"\",\n",
    "        }\n",
    "    ]\n",
    "    '''\n",
    "    text = _tokenizer.apply_chat_template(\n",
    "        _messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    model_inputs = _tokenizer([text], return_tensors=\"pt\").to(_model.device)\n",
    "\n",
    "    generated_ids = _model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=512\n",
    "    )\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "\n",
    "    response = _tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "565bdcb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "芙莉莲呀，她是我们世界里的一个角色，有着蓝色的长发和可爱的猫耳朵，性格活泼开朗，喜欢冒险和解谜，有时候会遇到一些奇怪的事情，但她总是能勇敢面对喵。\n"
     ]
    }
   ],
   "source": [
    "test_question='请问谁是 芙莉莲?'\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"你是用来回答二次元问题的 猫娘AI。每次回答请扮演猫娘, 即尽量用猫娘的语气回答, 句尾加上喵。使用中文时只使用中文来回答问题。\"},    # System prompt\n",
    "    {\"role\": \"user\", \"content\": test_question}, # User prompt\n",
    "]\n",
    "\n",
    "print(generate_response(model, tokenizer, messages))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47315d31",
   "metadata": {},
   "source": [
    "# Interpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab02809",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DO NOT MODIFY THIS CELL\n",
    "\n",
    "Python interpreter for executing code snippets and capturing their output.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import queue\n",
    "import signal\n",
    "import sys\n",
    "import time\n",
    "import traceback\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "from shutil import rmtree\n",
    "import shutil\n",
    "from multiprocessing import Process, Queue\n",
    "from typing import Hashable, cast\n",
    "\n",
    "import humanize\n",
    "import rich\n",
    "import shutup\n",
    "from rich.logging import RichHandler\n",
    "from rich.syntax import Syntax\n",
    "from dataclasses import dataclass\n",
    "from dataclasses_json import DataClassJsonMixin\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ExecutionResult(DataClassJsonMixin):\n",
    "    \"\"\"\n",
    "    Result of executing a code snippet in the interpreter.\n",
    "    Contains the output, execution time, and exception information.\n",
    "    \"\"\"\n",
    "    term_out: list[str]\n",
    "    exec_time: float\n",
    "    exc_type: str | None\n",
    "    exc_info: dict | None = None\n",
    "    exc_stack: list[tuple] | None = None\n",
    "\n",
    "def exception_summary(e, exec_file_name):\n",
    "    \"\"\"Generates a string that summarizes an exception and its stack trace\"\"\"\n",
    "    tb_lines = traceback.format_exception(e)\n",
    "    # Combine the traceback lines into a single string, skipping lines that contain \"importlib\".\n",
    "    tb_str = \"\".join(\n",
    "        [\n",
    "            line\n",
    "            for line in tb_lines\n",
    "            # if \"importlib\" not in line  # Filter out unwanted traceback lines.\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    exc_info = {}\n",
    "    if hasattr(e, \"args\"):\n",
    "        exc_info[\"args\"] = [str(i) for i in e.args]  # Store the exception arguments as strings.\n",
    "    for att in [\"name\", \"msg\", \"obj\"]:\n",
    "        if hasattr(e, att):\n",
    "            exc_info[att] = str(getattr(e, att))  # Store additional attributes if available.\n",
    "\n",
    "    tb = traceback.extract_tb(e.__traceback__)  # Extract the traceback information.\n",
    "    # Create a list of tuples for each frame in the traceback.\n",
    "    exc_stack = [(t.filename, t.lineno, t.name, t.line) for t in tb]\n",
    "\n",
    "    return tb_str, e.__class__.__name__, exc_info, exc_stack  # Return the formatted traceback and exception details.\n",
    "\n",
    "# Define a class that redirects write operations to a multiprocessing queue.\n",
    "class RedirectQueue:\n",
    "    def __init__(self, queue, timeout=5):\n",
    "        self.queue = queue  # Store the provided queue.\n",
    "        self.timeout = timeout  # Set the timeout for queue operations.\n",
    "\n",
    "    def write(self, msg):\n",
    "        try:\n",
    "            self.queue.put(msg, timeout=self.timeout)  # Attempt to put the message into the queue.\n",
    "        except queue.Full:\n",
    "            print.warning(\"Queue write timed out\")  # Warn if the queue is full and the write times out.\n",
    "\n",
    "    def flush(self):\n",
    "        pass  # No operation is needed for flushing in this context.\n",
    "\n",
    "# Define the Interpreter class that simulates a standalone Python REPL.\n",
    "class Interpreter:\n",
    "    def __init__(\n",
    "        self,\n",
    "        timeout: int = 3600,  # Default timeout of 3600 seconds.\n",
    "        agent_file_name: str = \"runfile.py\",  # Default file name for writing the agent's code.\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Simulates a standalone Python REPL with an execution time limit.\n",
    "\n",
    "        Args:\n",
    "            timeout (int, optional): Timeout for each code execution step. Defaults to 3600.\n",
    "            agent_file_name (str, optional): The name for the agent's code file. Defaults to \"runfile.py\".\n",
    "        \"\"\"\n",
    "        self.timeout = timeout  # Save the timeout value.\n",
    "        self.agent_file_name = agent_file_name  # Save the agent file name.\n",
    "        self.process: Process = None  # Initialize the process attribute (will hold the child process).\n",
    "\n",
    "    def child_proc_setup(self, result_outq: Queue) -> None:\n",
    "        # Import shutup to suppress warnings in the child process.\n",
    "        import shutup\n",
    "\n",
    "        shutup.mute_warnings()  # Mute all warnings before further execution.\n",
    "\n",
    "        # Redirect both stdout and stderr to the provided result queue.\n",
    "        # trunk-ignore(mypy/assignment)\n",
    "        sys.stdout = sys.stderr = RedirectQueue(result_outq)\n",
    "\n",
    "    def _run_session(\n",
    "        self, code_inq: Queue, result_outq: Queue, event_outq: Queue\n",
    "    ) -> None:\n",
    "        self.child_proc_setup(result_outq)  # Set up the child process for capturing output.\n",
    "\n",
    "        global_scope: dict = {}  # Create an empty dictionary to serve as the global scope.\n",
    "        while True:  # Continuously wait for new code to execute.\n",
    "            code = code_inq.get()  # Retrieve code from the code input queue.\n",
    "            with open(self.agent_file_name, \"w\") as f:  # Open the agent file for writing.\n",
    "                f.write(code)  # Write the received code into the file.\n",
    "\n",
    "            event_outq.put((\"state:ready\",))  # Signal that the interpreter is ready to execute the code.\n",
    "            try:\n",
    "                # Compile and execute the code within the global scope.\n",
    "                exec(compile(code, self.agent_file_name, \"exec\"), global_scope)\n",
    "            except BaseException as e:\n",
    "                # If an exception occurs, generate a summary of the exception.\n",
    "                tb_str, e_cls_name, exc_info, exc_stack = exception_summary(\n",
    "                    e,\n",
    "                    self.agent_file_name,\n",
    "                )\n",
    "                result_outq.put(tb_str)  # Put the traceback string into the result queue.\n",
    "                if e_cls_name == \"KeyboardInterrupt\":\n",
    "                    e_cls_name = \"TimeoutError\"  # Convert a KeyboardInterrupt into a TimeoutError.\n",
    "\n",
    "                event_outq.put((\"state:finished\", e_cls_name, exc_info, exc_stack))  # Signal that execution finished with an error.\n",
    "            else:\n",
    "                event_outq.put((\"state:finished\", None, None, None))  # Signal that execution finished successfully.\n",
    "\n",
    "            os.remove(self.agent_file_name)  # Remove the agent file after execution.\n",
    "\n",
    "            result_outq.put(\"<|EOF|>\")  # Put an EOF marker to indicate the end of output.\n",
    "\n",
    "    def create_process(self) -> None:\n",
    "        # Create three queues for communication with the child process:\n",
    "        # - code_inq: for sending code to execute.\n",
    "        # - result_outq: for receiving output from the execution.\n",
    "        # - event_outq: for receiving state events (like ready and finished).\n",
    "        # trunk-ignore(mypy/var-annotated)\n",
    "        self.code_inq, self.result_outq, self.event_outq = Queue(), Queue(), Queue()\n",
    "        self.process = Process(\n",
    "            target=self._run_session,  # Set the target function for the child process.\n",
    "            args=(self.code_inq, self.result_outq, self.event_outq),  # Provide the necessary queues as arguments.\n",
    "        )\n",
    "        self.process.start()  # Start the child process.\n",
    "\n",
    "    def cleanup_session(self):\n",
    "        if self.process is None:  # If there is no process, nothing to clean up.\n",
    "            return\n",
    "        try:\n",
    "            # Attempt to terminate the child process gracefully.\n",
    "            self.process.terminate()  # Request the process to terminate.\n",
    "            self.process.join(timeout=0.5)  # Wait for the process to finish with a 0.5-second timeout.\n",
    "\n",
    "            if self.process.exitcode is None:  # If the process is still running,\n",
    "                self.process.kill()  # Forcefully kill the process.\n",
    "                self.process.join(timeout=0.5)  # Wait again for termination.\n",
    "\n",
    "                if self.process.exitcode is None:  # If the process still hasn't terminated,\n",
    "                    os.kill(self.process.pid, signal.SIGKILL)  # Send a SIGKILL signal.\n",
    "        except Exception as e:\n",
    "            print(f\"Error during process cleanup: {e}\")  # Print an error message if cleanup fails.\n",
    "        finally:\n",
    "            if self.process is not None:  # If the process exists,\n",
    "                self.process.close()  # Close the process.\n",
    "                self.process = None  # Reset the process attribute to None.\n",
    "\n",
    "    def run(self, code: str, reset_session=True) -> ExecutionResult:\n",
    "        \"\"\"\n",
    "        Execute the provided Python command in a separate process and return its output.\n",
    "\n",
    "        Parameters:\n",
    "            code (str): Python code to execute.\n",
    "            reset_session (bool, optional): Whether to reset the interpreter session before executing the code. Defaults to True.\n",
    "\n",
    "        Returns:\n",
    "            ExecutionResult: Object containing the output and metadata of the code execution.\n",
    "        \"\"\"\n",
    "\n",
    "        if reset_session:\n",
    "            if self.process is not None:\n",
    "                # If a previous process exists, clean it up before starting a new one.\n",
    "                self.cleanup_session()\n",
    "            self.create_process()  # Create a new child process.\n",
    "        else:\n",
    "            # For the first execution, reset_session must be True.\n",
    "            assert self.process is not None\n",
    "\n",
    "        assert self.process.is_alive()  # Ensure that the child process is running.\n",
    "\n",
    "        self.code_inq.put(code)  # Send the code to the child process via the queue.\n",
    "\n",
    "        # Wait for the child process to signal that it is ready.\n",
    "        try:\n",
    "            state = self.event_outq.get(timeout=10)  # Wait up to 10 seconds for the \"state:ready\" event.\n",
    "        except queue.Empty:\n",
    "            msg = \"REPL child process failed to start execution\"\n",
    "            print.critical(msg)  # Log a critical error if the process does not start.\n",
    "            while not self.result_outq.empty():\n",
    "                continue  # Drain the result queue.\n",
    "            raise RuntimeError(msg) from None\n",
    "        assert state[0] == \"state:ready\", state  # Verify that the received state is \"state:ready\".\n",
    "        start_time = time.time()  # Record the start time of execution.\n",
    "\n",
    "        child_in_overtime = False  # Flag to indicate if the child process has exceeded the timeout.\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                # Try to get the finished state from the child process.\n",
    "                state = self.event_outq.get(timeout=1)  # Wait for the \"state:finished\" event.\n",
    "                assert state[0] == \"state:finished\", state  # Ensure the state is \"state:finished\".\n",
    "                exec_time = time.time() - start_time  # Calculate the total execution time.\n",
    "                break  # Exit the loop if execution is finished.\n",
    "            except queue.Empty:\n",
    "                # If no event is received, check whether the process is still alive.\n",
    "                if not child_in_overtime and not self.process.is_alive():\n",
    "                    msg = \"REPL child process died unexpectedly\"\n",
    "                    raise RuntimeError(msg) from None\n",
    "\n",
    "                # If the process is still running, check if it has exceeded the timeout.\n",
    "                if self.timeout is None:\n",
    "                    continue\n",
    "                running_time = time.time() - start_time  # Determine the running time.\n",
    "                if running_time > self.timeout:\n",
    "                    print(f\"Execution exceeded timeout of {self.timeout}s\")  # Log a timeout message.\n",
    "                    os.kill(self.process.pid, signal.SIGINT)  # Send SIGINT to the process.\n",
    "                    child_in_overtime = True  # Mark that the process is now in overtime.\n",
    "\n",
    "                    # If the process exceeds the timeout by more than 5 seconds, force cleanup.\n",
    "                    if running_time > self.timeout + 5:\n",
    "                        self.cleanup_session()  # Clean up the child process.\n",
    "\n",
    "                        state = (None, \"TimeoutError\", {}, [])  # Set state to indicate a timeout error.\n",
    "                        exec_time = self.timeout  # Set the execution time to the timeout limit.\n",
    "                        break\n",
    "\n",
    "        output: list[str] = []  # Initialize a list to collect output lines.\n",
    "        # Collect all output from the result queue until the EOF marker is encountered.\n",
    "        start_collect = time.time()  # Record the start time for output collection.\n",
    "        while not self.result_outq.empty() or not output or output[-1] != \"<|EOF|>\":\n",
    "            try:\n",
    "                # If output collection exceeds 5 seconds, log a warning.\n",
    "                if time.time() - start_collect > 5:\n",
    "                    print.warning(\"Output collection timed out\")\n",
    "                    break\n",
    "                output.append(self.result_outq.get(timeout=1))  # Append the next line of output.\n",
    "            except queue.Empty:\n",
    "                continue  # Continue if no output is available immediately.\n",
    "        output.pop()  # Remove the EOF marker from the output list.\n",
    "\n",
    "        # Extract exception information from the finished state.\n",
    "        e_cls_name, exc_info, exc_stack = state[1:]\n",
    "\n",
    "        if e_cls_name == \"TimeoutError\":\n",
    "            # Append a timeout error message to the output if a timeout occurred.\n",
    "            output.append(\n",
    "                f\"TimeoutError: Execution exceeded the time limit of {humanize.naturaldelta(self.timeout)}\"\n",
    "            )\n",
    "        else:\n",
    "            # Append the execution time information to the output.\n",
    "            output.append(\n",
    "                f\"Execution time: {humanize.naturaldelta(exec_time)} seconds (time limit is {humanize.naturaldelta(self.timeout)}).\"\n",
    "            )\n",
    "        # Return an ExecutionResult object with all the execution details.\n",
    "        return ExecutionResult(output, exec_time, e_cls_name, exc_info, exc_stack)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7bcde0",
   "metadata": {},
   "source": [
    "# Text Processing Util Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "64a0150c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def wrap_code(code: str, lang=\"python\") -> str:\n",
    "    \"\"\"Wraps code with three backticks.\"\"\"\n",
    "    return f\"```{lang}\\n{code}\\n```\"\n",
    "\n",
    "\n",
    "def is_valid_python_script(script):\n",
    "    \"\"\"Check if a script is a valid Python script.\"\"\"\n",
    "    try:\n",
    "        compile(script, \"<string>\", \"exec\")\n",
    "        return True\n",
    "    except SyntaxError:\n",
    "        return False\n",
    "\n",
    "\n",
    "def extract_jsons(text):\n",
    "    \"\"\"Extract all JSON objects from the text. Caveat: This function cannot handle nested JSON objects.\"\"\"\n",
    "    json_objects = []\n",
    "\n",
    "    # Find {} by regular expression\n",
    "    matches = re.findall(r\"\\{.*?\\}\", text, re.DOTALL)\n",
    "\n",
    "    # Try to transform string into json objects\n",
    "    for match in matches:\n",
    "        try:\n",
    "            json_obj = json.loads(match)\n",
    "            json_objects.append(json_obj)\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "\n",
    "    return json_objects\n",
    "\n",
    "def trim_long_string(string, threshold=5100, k=2500):\n",
    "    # Check if the length of the string is longer than the threshold\n",
    "    if len(string) > threshold:\n",
    "        # Output the first k and last k characters\n",
    "        first_k_chars = string[:k]\n",
    "        last_k_chars = string[-k:]\n",
    "\n",
    "        truncated_len = len(string) - 2 * k\n",
    "\n",
    "        return f\"{first_k_chars}\\n ... [{truncated_len} characters truncated] ... \\n{last_k_chars}\"\n",
    "    else:\n",
    "        return string\n",
    "\n",
    "def extract_code(text):\n",
    "    \"\"\"Extract python code blocks from the text.\"\"\"\n",
    "    parsed_codes = []\n",
    "\n",
    "    # When code is in a text or python block\n",
    "    matches = re.findall(r\"```(python)?\\n*(.*?)\\n*```\", text, re.DOTALL)\n",
    "    for match in matches:\n",
    "        code_block = match[1]\n",
    "        parsed_codes.append(code_block)\n",
    "\n",
    "    # When the entire text is code or backticks of the code block is missing\n",
    "    if len(parsed_codes) == 0:\n",
    "        matches = re.findall(r\"^(```(python)?)?\\n?(.*?)\\n?(```)?$\", text, re.DOTALL)\n",
    "        if matches:\n",
    "            code_block = matches[0][2]\n",
    "            parsed_codes.append(code_block)\n",
    "\n",
    "    # validate the parsed codes\n",
    "    valid_code_blocks = [\n",
    "        c for c in parsed_codes if is_valid_python_script(c)\n",
    "    ]\n",
    "    return \"\\n\\n\".join(valid_code_blocks)\n",
    "\n",
    "def extract_text_up_to_code(s):\n",
    "    \"\"\"Extract (presumed) natural language text up to the start of the first code block.\"\"\"\n",
    "    if \"```\" not in s:\n",
    "        return \"\"\n",
    "    return s[: s.find(\"```\")].strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032b6d6d",
   "metadata": {},
   "source": [
    "# Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fc4200d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import uuid\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Literal, Optional\n",
    "\n",
    "from dataclasses_json import DataClassJsonMixin\n",
    "\n",
    "\n",
    "@dataclass(eq=False)\n",
    "class Node(DataClassJsonMixin):\n",
    "    \"\"\"A single node in the solution tree. Contains code, execution results, and evaluation information.\"\"\"\n",
    "\n",
    "    # ---- code & plan ----\n",
    "    code: str\n",
    "    plan: str = field(default=None, kw_only=True)  # type: ignore\n",
    "\n",
    "    # ---- general attrs ----\n",
    "    step: int = field(default=None, kw_only=True)  # type: ignore\n",
    "    id: str = field(default_factory=lambda: uuid.uuid4().hex, kw_only=True)\n",
    "    ctime: float = field(default_factory=lambda: time.time(), kw_only=True)\n",
    "    parent: Optional[\"Node\"] = field(default=None, kw_only=True)\n",
    "    children: set[\"Node\"] = field(default_factory=set, kw_only=True)\n",
    "\n",
    "    # ---- execution info ----\n",
    "    _term_out: list[str] = field(default=None, kw_only=True)  # type: ignore\n",
    "    exec_time: float = field(default=None, kw_only=True)  # type: ignore\n",
    "    exc_type: str | None = field(default=None, kw_only=True)\n",
    "    exc_info: dict | None = field(default=None, kw_only=True)\n",
    "    exc_stack: list[tuple] | None = field(default=None, kw_only=True)\n",
    "\n",
    "    # ---- evaluation ----\n",
    "    # post-execution result analysis (findings/feedback)\n",
    "    analysis: str = field(default=None, kw_only=True)  # type: ignore\n",
    "    metric: float = field(default=None, kw_only=True)  # type: ignore\n",
    "    # whether the agent decided that the code is buggy\n",
    "    # -> always True if exc_type is not None or no valid metric\n",
    "    is_buggy: bool = field(default=None, kw_only=True)  # type: ignore\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        if self.parent is not None:\n",
    "            self.parent.children.add(self)\n",
    "\n",
    "    @property\n",
    "    def stage_name(self) -> Literal[\"draft\", \"debug\", \"improve\"]:\n",
    "        \"\"\"\n",
    "        Return the stage of the node:\n",
    "        - \"stage\" if the node is an initial solution draft\n",
    "        - \"debug\" if the node is the result of a debugging step\n",
    "        - \"improve\" if the node is the result of an improvement step\n",
    "        \"\"\"\n",
    "        if self.parent is None:\n",
    "            return \"draft\"\n",
    "        return \"debug\" if self.parent.is_buggy else \"improve\"\n",
    "\n",
    "    def absorb_exec_result(self, exec_result: ExecutionResult):\n",
    "        \"\"\"Absorb the result of executing the code from this node.\"\"\"\n",
    "        self._term_out = exec_result.term_out\n",
    "        self.exec_time = exec_result.exec_time\n",
    "        self.exc_type = exec_result.exc_type\n",
    "        self.exc_info = exec_result.exc_info\n",
    "        self.exc_stack = exec_result.exc_stack\n",
    "\n",
    "    @property\n",
    "    def term_out(self) -> str:\n",
    "        \"\"\"Get the terminal output of the code execution (after truncating it).\"\"\"\n",
    "        return trim_long_string(\"\".join(self._term_out))\n",
    "\n",
    "    @property\n",
    "    def is_leaf(self) -> bool:\n",
    "        \"\"\"Check if the node is a leaf node in the solution tree.\"\"\"\n",
    "        return not self.children\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return isinstance(other, Node) and self.id == other.id\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.id)\n",
    "\n",
    "    @property\n",
    "    def debug_depth(self) -> int:\n",
    "        \"\"\"\n",
    "        Length of the current debug path\n",
    "        - 0 if the node is not a debug node (parent is not buggy)\n",
    "        - 1 if the parent is buggy but the skip parent isn't\n",
    "        - n if there were n consecutive debugging steps\n",
    "        \"\"\"\n",
    "        if self.stage_name != \"debug\":\n",
    "            return 0\n",
    "        return self.parent.debug_depth + 1  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd91a1b",
   "metadata": {},
   "source": [
    "# Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "30f38983",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Journal(DataClassJsonMixin):\n",
    "    \"\"\"A collection of nodes representing the solution tree.\"\"\"\n",
    "\n",
    "    nodes: list[Node] = field(default_factory=list)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Node:\n",
    "        return self.nodes[idx]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Return the number of nodes in the journal.\"\"\"\n",
    "        return len(self.nodes)\n",
    "\n",
    "    def append(self, node: Node) -> None:\n",
    "        \"\"\"Append a new node to the journal.\"\"\"\n",
    "        node.step = len(self.nodes)\n",
    "        self.nodes.append(node)\n",
    "\n",
    "    @property\n",
    "    def draft_nodes(self) -> list[Node]:\n",
    "        \"\"\"Return a list of nodes representing intial coding drafts\"\"\"\n",
    "        return [n for n in self.nodes if n.parent is None]\n",
    "\n",
    "    @property\n",
    "    def buggy_nodes(self) -> list[Node]:\n",
    "        \"\"\"Return a list of nodes that are considered buggy by the agent.\"\"\"\n",
    "        return [n for n in self.nodes if n.is_buggy]\n",
    "\n",
    "    @property\n",
    "    def good_nodes(self) -> list[Node]:\n",
    "        \"\"\"Return a list of nodes that are not considered buggy by the agent.\"\"\"\n",
    "        return [n for n in self.nodes if not n.is_buggy]\n",
    "\n",
    "    def get_metric_history(self) -> list[float]:\n",
    "        \"\"\"Return a list of all metric values in the journal.\"\"\"\n",
    "        return [n.metric for n in self.nodes]\n",
    "\n",
    "    def get_good_nodes(self) -> Node:\n",
    "        return [n for n in self.nodes if not n.is_buggy]\n",
    "\n",
    "    def get_best_node(self, only_good=True) -> None | Node:\n",
    "        \"\"\"Return the best solution found so far (node with the highest validation metric).\"\"\"\n",
    "        if only_good:\n",
    "            nodes = self.good_nodes\n",
    "            if not nodes:\n",
    "                return None\n",
    "        else:\n",
    "            nodes = self.nodes\n",
    "        return min(nodes, key=lambda n: n.metric)\n",
    "\n",
    "    def generate_summary(self, include_code: bool = False) -> str:\n",
    "        \"\"\"Generate a summary of the journal for the agent.\"\"\"\n",
    "        summary = []\n",
    "        for n in self.good_nodes:\n",
    "            summary_part = f\"Design: {n.plan}\\n\"\n",
    "            if include_code:\n",
    "                summary_part += f\"Code: {n.code}\\n\"\n",
    "            summary_part += f\"Results: {n.analysis}\\n\"\n",
    "            summary_part += f\"Validation Metric (Mean Squared Error): {n.metric}\\n\"\n",
    "            summary.append(summary_part)\n",
    "        return \"\\n-------------------------------\\n\".join(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52dd96f3",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "989104fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import Any, Callable, cast\n",
    "\n",
    "import re\n",
    "import sys\n",
    "import json\n",
    "import humanize\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "ExecCallbackType = Callable[[str, bool], ExecutionResult]\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        cfg,\n",
    "        journal: Journal,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.journal = journal\n",
    "        self.data_preview: str | None = None\n",
    "\n",
    "    def search_policy(self) -> Node | None:\n",
    "        \"\"\"Select a node to work on (or None to draft a new node).\"\"\"\n",
    "        search_cfg = self.cfg.agent.search\n",
    "\n",
    "        # initial drafting\n",
    "        if len(self.journal.draft_nodes) < search_cfg.num_drafts:\n",
    "            return None\n",
    "\n",
    "        # debugging\n",
    "        if random.random() < search_cfg.debug_prob:\n",
    "            # nodes that are buggy + leaf nodes + debug depth < max debug depth\n",
    "            debuggable_nodes = [\n",
    "                n\n",
    "                for n in self.journal.buggy_nodes\n",
    "                if n.is_leaf\n",
    "            ]\n",
    "            if debuggable_nodes:\n",
    "                return random.choice(debuggable_nodes)\n",
    "\n",
    "\n",
    "        # back to drafting if no nodes to improve\n",
    "        good_nodes = self.journal.good_nodes\n",
    "        if not good_nodes:\n",
    "            return None\n",
    "\n",
    "        # greedy\n",
    "        greedy_node = self.journal.get_best_node()\n",
    "\n",
    "        return greedy_node\n",
    "\n",
    "\n",
    "    def plan_and_code_query(self, system_message, user_message, retries=3) -> tuple[str, str]:\n",
    "        \"\"\"Generate a natural language plan + code in the same LLM call and split them apart.\"\"\"\n",
    "        completion_text = None\n",
    "        for _ in range(retries):\n",
    "\n",
    "            response = generate_response(\n",
    "                model,\n",
    "                tokenizer,\n",
    "                _messages=[\n",
    "                    {'role': 'system', \"content\": system_message},\n",
    "                    {'role': 'user', \"content\": user_message}\n",
    "                ]\n",
    "            )\n",
    "            completion_text = response\n",
    "            code = extract_code(completion_text)\n",
    "            nl_text = extract_text_up_to_code(completion_text)\n",
    "\n",
    "            if code:\n",
    "                return nl_text, code\n",
    "\n",
    "            print(\"Plan + code extraction failed, retrying...\")\n",
    "        print(\"Final plan + code extraction attempt failed, giving up...\")\n",
    "        return \"\", completion_text\n",
    "\n",
    "    def _draft(self) -> Node:\n",
    "\n",
    "        # ================ TODO: ask LLM agents to come up with a solution and then implement ================\n",
    "\n",
    "        system_prompt = \"You are an AI agent.\"\n",
    "\n",
    "        user_prompt = [\n",
    "            \"You have to come up with a solution for machine learning task and then implement this solution in Python.\"\n",
    "            f\"The task is to {str(self.cfg.task_goal)} \",\n",
    "            f'All the provided input data is stored in \"{self.cfg.data_dir}\" directory.',\n",
    "            f\"{str(self.data_preview)}\",\n",
    "            f'You have to save the predictions result on testing set in \"{Path(\"./ML2025Spring-hw2-public\").resolve()}/submission.csv\".',\n",
    "            'Note that the testing file DOES NOT have the target column.'\n",
    "        ]\n",
    "\n",
    "        system_message = system_prompt\n",
    "        user_message = \"\\n\".join(user_prompt)\n",
    "        plan, code = self.plan_and_code_query(system_message=system_message, user_message=user_message)\n",
    "        return Node(plan=plan, code=code)\n",
    "\n",
    "    def _improve(self, parent_node: Node) -> Node:\n",
    "\n",
    "        # ================  TODO: ask LLM agent to improve drafts ================\n",
    "\n",
    "        system_prompt = \"You are an AI assistant. Please improve the program based on the existing program and the training results of the machine learning task.\"\n",
    "\n",
    "        user_prompt = [\n",
    "            f\"Task description: {str(self.cfg.task_goal)} \"\n",
    "            f\"Memory: {str(self.journal.generate_summary())} \"\n",
    "            f\"Previous solution: Code: {str(wrap_code(parent_node.code))} \"\n",
    "        ]\n",
    "        system_message = system_prompt\n",
    "        user_message = \" \".join(user_prompt)\n",
    "        plan, code = self.plan_and_code_query(system_message=system_message, user_message=user_message)\n",
    "        return Node(plan=plan, code=code, parent=parent_node)\n",
    "\n",
    "    def _debug(self, parent_node: Node) -> Node:\n",
    "\n",
    "        # ================  TODO: ask LLM agent to debug ================\n",
    "        system_prompt = \"You are an AI agent. The current program has bugs and needs your improvement.\"\n",
    "\n",
    "\n",
    "        user_prompt = [\n",
    "            f\"Task description: {str(self.cfg.task_goal)}\\n\\n\",\n",
    "            f\"Previous (buggy) implementation: {str(wrap_code(parent_node.code))}\\n\\n\",\n",
    "            f\"Execution output: {str(wrap_code(parent_node.term_out, lang=''))}\\n\\n\",\n",
    "            str(self.data_preview)\n",
    "        ]\n",
    "\n",
    "        system_message = system_prompt\n",
    "        user_message = \" \".join(user_prompt)\n",
    "\n",
    "        plan, code = self.plan_and_code_query(system_message=system_message, user_message=user_message)\n",
    "        return Node(plan=plan, code=code, parent=parent_node)\n",
    "\n",
    "    def update_data_preview(\n",
    "        self,\n",
    "    ):\n",
    "        self.data_preview = data_preview_generate(cfg.data_dir)\n",
    "\n",
    "    def step(self, exec_callback: ExecCallbackType):\n",
    "        if not self.journal.nodes or self.data_preview is None:\n",
    "            self.update_data_preview()\n",
    "\n",
    "        parent_node = self.search_policy()\n",
    "\n",
    "        if parent_node is None:\n",
    "            result_node = self._draft()\n",
    "        elif parent_node.is_buggy:\n",
    "            result_node = self._debug(parent_node)\n",
    "        else:\n",
    "            result_node = self._improve(parent_node)\n",
    "\n",
    "        self.parse_exec_result(\n",
    "            node=result_node,\n",
    "            exec_result=exec_callback(result_node.code, True),\n",
    "        )\n",
    "        self.journal.append(result_node)\n",
    "\n",
    "    def parse_exec_result(self, node: Node, exec_result: ExecutionResult):\n",
    "        node.absorb_exec_result(exec_result)\n",
    "\n",
    "        system_prompt = \"You are an AI assistant. You need to evaluate the code for a machine learning task based on its training score.\"\n",
    "\n",
    "        # ================  TODO: ask LLM agent to extract evaluation result from the execution output. ================\n",
    "        format_instruction = \"\"\"\n",
    "        Please analyze the execution output and answer in strict JSON format with the following keys:\n",
    "        - \"summary\": (string) A brief analysis of the code execution and performance.\n",
    "        - \"is_buggy\": (boolean) True if the code failed, reported an error, or the score is 0. False otherwise.\n",
    "        - \"metric\": (float) The final evaluation metric extracted from the output (e.g., accuracy, loss, reward). If failed, set to 0.0.\n",
    "        \n",
    "        Do not add any explanations outside the JSON object.\n",
    "        \"\"\"\n",
    "\n",
    "        # save log file\n",
    "        user_prompt = f\"\"\"\n",
    "            {format_instruction}\n",
    "\n",
    "            The task is:\n",
    "            {self.cfg.task_goal}\n",
    "\n",
    "            The code implementation is:\n",
    "            {wrap_code(node.code)}\n",
    "\n",
    "            The execution output is:\n",
    "            {wrap_code(node.term_out, lang=\"\")}\n",
    "        \"\"\"\n",
    "\n",
    "        system_message = system_prompt\n",
    "        user_message = user_prompt\n",
    "\n",
    "        response = generate_response(\n",
    "            model, tokenizer,\n",
    "            _messages=[\n",
    "                {'role': 'system', \"content\": system_message},\n",
    "                {'role': 'user', \"content\": user_message}\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # ================  TODO: evaluation ================\n",
    "        # you can force the LLM to structure the output to extract the metric\n",
    "        # reference: https://python.useinstructor.com/integrations/llama-cpp-python/#llama-cpp-python\n",
    "        # node.analysis = response.summary\n",
    "        # node.is_buggy = (\n",
    "        #     response.is_buggy\n",
    "        #     or node.exc_type is not None\n",
    "        #     or response.metric is None\n",
    "        # )\n",
    "        try:\n",
    "            # 1. 获取响应内容 (假设 response 是字符串，如果是对象请用 response.content)\n",
    "            content = response if isinstance(response, str) else getattr(response, 'content', str(response))\n",
    "\n",
    "            # 2. 清洗数据：LLM 经常会把 JSON 包裹在 Markdown 代码块中 (```json ... ```)\n",
    "            # 使用正则提取第一个 { ... } 之间的内容\n",
    "            json_match = re.search(r'\\{.*\\}', content, re.DOTALL)\n",
    "            if json_match:\n",
    "                json_str = json_match.group(0)\n",
    "                data = json.loads(json_str)\n",
    "            else:\n",
    "                # 尝试直接解析\n",
    "                data = json.loads(content)\n",
    "\n",
    "            # 3. 赋值给 Node\n",
    "            node.analysis = data.get(\"summary\", \"No summary provided.\")\n",
    "            # 如果 node 本身有异常(exc_type) 或者 LLM 认为 buggy，则标记为 buggy\n",
    "            node.is_buggy = data.get(\"is_buggy\", False) or (node.exc_type is not None)\n",
    "            node.metric = float(data.get(\"metric\", 0.0))\n",
    "\n",
    "        except (json.JSONDecodeError, ValueError) as e:\n",
    "            # 解析失败兜底逻辑\n",
    "            print(f\"Error parsing LLM response: {e}, Response: {content}\")\n",
    "            node.analysis = \"Failed to parse LLM evaluation.\"\n",
    "            node.is_buggy = True\n",
    "            node.metric = 0.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20dd8ed",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf696fa",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "\n",
    "查看数据特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2aa871d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import humanize\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def preview_csv(p: Path) -> str:\n",
    "    \"\"\"Generate a textual preview of a csv file\"\"\"\n",
    "\n",
    "    df = pd.read_csv(p)\n",
    "\n",
    "    out = []\n",
    "\n",
    "    out.append(f\"-> {str(p)} has {df.shape[0]} rows and {df.shape[1]} columns.\")\n",
    "\n",
    "    # ================  TODO: Tell LLM agents which feature is useful for prediction ================\n",
    "\n",
    "    cols = df.columns.tolist()\n",
    "    cols_str = \", \".join(cols)\n",
    "    res = f\"The columns are: {cols_str}\"\n",
    "\n",
    "    out.append(res)\n",
    "\n",
    "    return \"\\n\".join(out)\n",
    "\n",
    "def data_preview_generate(base_path):\n",
    "    \"\"\"\n",
    "    Generate a textual preview of a directory\n",
    "    \"\"\"\n",
    "\n",
    "    result = []\n",
    "    files = [p for p in Path(base_path).iterdir()]\n",
    "    for f in sorted(files):\n",
    "        result.append(preview_csv(f))\n",
    "\n",
    "    result = \"\\n\\n\".join(result)\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696cb91e",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f8b27776",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# DO NOT MODIFY THIS CELL\n",
    "class Config:\n",
    "    \"\"\"\n",
    "    A recursive configuration class that converts a dictionary into an object\n",
    "    with attributes accessible using dot notation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dictionary):\n",
    "        for key, value in dictionary.items():\n",
    "            if isinstance(value, dict):\n",
    "                value = Config(value)\n",
    "            setattr(self, key, value)\n",
    "\n",
    "def set_seed(seed=531):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed()\n",
    "\n",
    "# ================  TODO: config ================\n",
    "config = {\n",
    "    # experiment configurations\n",
    "    \"exp_name\": \"ML2025_HW2\",\n",
    "    \"data_dir\":  Path(\"./ML2025Spring-hw2-public\").resolve(),\n",
    "\n",
    "    # the description of the task\n",
    "    \"task_goal\": \"Given the survey results from the past two days in a specific state in the U.S.,\\\n",
    "                  predict the probability of testing positive on day 3. \\\n",
    "                  The evaluation metric is Mean Squared Error (MSE).\",\n",
    "\n",
    "    \"agent\": {\n",
    "        # the number of iterations\n",
    "        \"steps\": 5,\n",
    "        \"search\": {\n",
    "            # decide whether to debug or improve\n",
    "            \"debug_prob\": 0.5,\n",
    "            # the number of draft generated before improving/debugging\n",
    "            \"num_drafts\": 1,\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "cfg = Config(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1d574e",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7f399217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plan + code extraction failed, retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plan + code extraction failed, retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plan + code extraction failed, retrying...\n",
      "Plan + code extraction failed, retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plan + code extraction failed, retrying...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 43\u001b[39m\n\u001b[32m     39\u001b[39m     interpreter.cleanup_session()\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 31\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     28\u001b[39m global_step = \u001b[38;5;28mlen\u001b[39m(journal)\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m global_step < cfg.agent.steps:\n\u001b[32m     30\u001b[39m     \u001b[38;5;66;03m# run agent\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexec_callback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexec_callback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m     \u001b[38;5;66;03m# save results for this iteration\u001b[39;00m\n\u001b[32m     33\u001b[39m     save_run(cfg, journal)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 149\u001b[39m, in \u001b[36mAgent.step\u001b[39m\u001b[34m(self, exec_callback)\u001b[39m\n\u001b[32m    147\u001b[39m     result_node = \u001b[38;5;28mself\u001b[39m._draft()\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m parent_node.is_buggy:\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m     result_node = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_debug\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparent_node\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    150\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    151\u001b[39m     result_node = \u001b[38;5;28mself\u001b[39m._improve(parent_node)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 132\u001b[39m, in \u001b[36mAgent._debug\u001b[39m\u001b[34m(self, parent_node)\u001b[39m\n\u001b[32m    129\u001b[39m system_message = system_prompt\n\u001b[32m    130\u001b[39m user_message = \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m.join(user_prompt)\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m plan, code = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mplan_and_code_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43msystem_message\u001b[49m\u001b[43m=\u001b[49m\u001b[43msystem_message\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_message\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_message\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Node(plan=plan, code=code, parent=parent_node)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 61\u001b[39m, in \u001b[36mAgent.plan_and_code_query\u001b[39m\u001b[34m(self, system_message, user_message, retries)\u001b[39m\n\u001b[32m     58\u001b[39m completion_text = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(retries):\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m     response = \u001b[43mgenerate_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_messages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msystem\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem_message\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_message\u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m     completion_text = response\n\u001b[32m     70\u001b[39m     code = extract_code(completion_text)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mgenerate_response\u001b[39m\u001b[34m(_model, _tokenizer, _messages)\u001b[39m\n\u001b[32m     16\u001b[39m text = _tokenizer.apply_chat_template(\n\u001b[32m     17\u001b[39m     _messages,\n\u001b[32m     18\u001b[39m     tokenize=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     19\u001b[39m     add_generation_prompt=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     20\u001b[39m )\n\u001b[32m     21\u001b[39m model_inputs = _tokenizer([text], return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m).to(_model.device)\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m generated_ids = \u001b[43m_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m512\u001b[39;49m\n\u001b[32m     26\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m generated_ids = [\n\u001b[32m     28\u001b[39m     output_ids[\u001b[38;5;28mlen\u001b[39m(input_ids):] \u001b[38;5;28;01mfor\u001b[39;00m input_ids, output_ids \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(model_inputs.input_ids, generated_ids)\n\u001b[32m     29\u001b[39m ]\n\u001b[32m     31\u001b[39m response = _tokenizer.batch_decode(generated_ids, skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/vLLM/vllm_env/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/vLLM/vllm_env/lib/python3.11/site-packages/transformers/generation/utils.py:2564\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2561\u001b[39m model_kwargs[\u001b[33m\"\u001b[39m\u001b[33muse_cache\u001b[39m\u001b[33m\"\u001b[39m] = generation_config.use_cache\n\u001b[32m   2563\u001b[39m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2564\u001b[39m result = \u001b[43mdecoding_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2565\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2566\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2567\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2568\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2569\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2570\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgeneration_mode_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2571\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2572\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2574\u001b[39m \u001b[38;5;66;03m# Convert to legacy cache format if requested\u001b[39;00m\n\u001b[32m   2575\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2576\u001b[39m     generation_config.return_legacy_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2577\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2578\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result.past_key_values, \u001b[33m\"\u001b[39m\u001b[33mto_legacy_cache\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2579\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/vLLM/vllm_env/lib/python3.11/site-packages/transformers/generation/utils.py:2787\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   2785\u001b[39m     is_prefill = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   2786\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2787\u001b[39m     outputs = \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   2789\u001b[39m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[32m   2790\u001b[39m model_kwargs = \u001b[38;5;28mself\u001b[39m._update_model_kwargs_for_generation(\n\u001b[32m   2791\u001b[39m     outputs,\n\u001b[32m   2792\u001b[39m     model_kwargs,\n\u001b[32m   2793\u001b[39m     is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2794\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/vLLM/vllm_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/vLLM/vllm_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/vLLM/vllm_env/lib/python3.11/site-packages/accelerate/hooks.py:175\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    173\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/vLLM/vllm_env/lib/python3.11/site-packages/transformers/utils/generic.py:918\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    916\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    917\u001b[39m     return_dict = return_dict_passed\n\u001b[32m--> \u001b[39m\u001b[32m918\u001b[39m output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    920\u001b[39m     output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/vLLM/vllm_env/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py:449\u001b[39m, in \u001b[36mQwen2ForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    417\u001b[39m \u001b[38;5;129m@can_return_tuple\u001b[39m\n\u001b[32m    418\u001b[39m \u001b[38;5;129m@auto_docstring\u001b[39m\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m   (...)\u001b[39m\u001b[32m    430\u001b[39m     **kwargs: Unpack[TransformersKwargs],\n\u001b[32m    431\u001b[39m ) -> CausalLMOutputWithPast:\n\u001b[32m    432\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    433\u001b[39m \u001b[33;03m    Example:\u001b[39;00m\n\u001b[32m    434\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    447\u001b[39m \u001b[33;03m    \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\u001b[39;00m\n\u001b[32m    448\u001b[39m \u001b[33;03m    ```\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     outputs: BaseModelOutputWithPast = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    450\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    452\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    453\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    454\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    455\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    456\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    458\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    460\u001b[39m     hidden_states = outputs.last_hidden_state\n\u001b[32m    461\u001b[39m     \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/vLLM/vllm_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/vLLM/vllm_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/vLLM/vllm_env/lib/python3.11/site-packages/transformers/utils/generic.py:1072\u001b[39m, in \u001b[36mcheck_model_inputs.<locals>.wrapped_fn.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1069\u001b[39m                 monkey_patched_layers.append((module, original_forward))\n\u001b[32m   1071\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1072\u001b[39m     outputs = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1073\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m original_exception:\n\u001b[32m   1074\u001b[39m     \u001b[38;5;66;03m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[39;00m\n\u001b[32m   1075\u001b[39m     \u001b[38;5;66;03m# Get a TypeError even after removing the recordable kwargs -> re-raise the original exception\u001b[39;00m\n\u001b[32m   1076\u001b[39m     \u001b[38;5;66;03m# Otherwise -> we're probably missing `**kwargs` in the decorated function\u001b[39;00m\n\u001b[32m   1077\u001b[39m     kwargs_without_recordable = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m recordable_keys}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/vLLM/vllm_env/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py:384\u001b[39m, in \u001b[36mQwen2Model.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, cache_position, **kwargs)\u001b[39m\n\u001b[32m    381\u001b[39m position_embeddings = \u001b[38;5;28mself\u001b[39m.rotary_emb(hidden_states, position_ids)\n\u001b[32m    383\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m decoder_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers[: \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers]:\n\u001b[32m--> \u001b[39m\u001b[32m384\u001b[39m     hidden_states = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    385\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    386\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask_mapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdecoder_layer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mattention_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    389\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    392\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    393\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    395\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.norm(hidden_states)\n\u001b[32m    396\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m BaseModelOutputWithPast(\n\u001b[32m    397\u001b[39m     last_hidden_state=hidden_states,\n\u001b[32m    398\u001b[39m     past_key_values=past_key_values \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    399\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/vLLM/vllm_env/lib/python3.11/site-packages/transformers/modeling_layers.py:94\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         logger.warning_once(message)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/vLLM/vllm_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/vLLM/vllm_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/vLLM/vllm_env/lib/python3.11/site-packages/accelerate/hooks.py:175\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    173\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/vLLM/vllm_env/lib/python3.11/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/vLLM/vllm_env/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py:249\u001b[39m, in \u001b[36mQwen2DecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_ids, past_key_values, use_cache, cache_position, position_embeddings, **kwargs)\u001b[39m\n\u001b[32m    247\u001b[39m residual = hidden_states\n\u001b[32m    248\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.post_attention_layernorm(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m249\u001b[39m hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    250\u001b[39m hidden_states = residual + hidden_states\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/vLLM/vllm_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/vLLM/vllm_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/vLLM/vllm_env/lib/python3.11/site-packages/accelerate/hooks.py:175\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    173\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/vLLM/vllm_env/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py:46\u001b[39m, in \u001b[36mQwen2MLP.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     down_proj = \u001b[38;5;28mself\u001b[39m.down_proj(\u001b[38;5;28mself\u001b[39m.act_fn(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgate_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m) * \u001b[38;5;28mself\u001b[39m.up_proj(x))\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/vLLM/vllm_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/vLLM/vllm_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/vLLM/vllm_env/lib/python3.11/site-packages/accelerate/hooks.py:175\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    173\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/vLLM/vllm_env/lib/python3.11/site-packages/torch/nn/modules/linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Define a function to save the best solution and other good solutions to files.\n",
    "def save_run(cfg, journal):\n",
    "    # Retrieve and save the best found solution.\n",
    "    best_node = journal.get_best_node(only_good=False)  # Get the best node.\n",
    "    with open(\"best_solution.py\", \"w\") as f:\n",
    "        f.write(best_node.code)\n",
    "\n",
    "    good_nodes = journal.get_good_nodes()  # Retrieve all good solution nodes.\n",
    "    for i, node in enumerate(good_nodes):\n",
    "        filename = f\"good_solution_{i}.py\"\n",
    "        with open(filename, \"w\") as f:\n",
    "            f.write(node.code)\n",
    "\n",
    "def main():\n",
    "\n",
    "    def exec_callback(*args, **kwargs):\n",
    "        res = interpreter.run(*args, **kwargs)\n",
    "        return res\n",
    "\n",
    "    journal = Journal()\n",
    "    agent = Agent(\n",
    "        cfg=cfg,\n",
    "        journal=journal,\n",
    "    )\n",
    "\n",
    "    interpreter = Interpreter()\n",
    "\n",
    "    global_step = len(journal)\n",
    "    while global_step < cfg.agent.steps:\n",
    "        # run agent\n",
    "        agent.step(exec_callback=exec_callback)\n",
    "        # save results for this iteration\n",
    "        save_run(cfg, journal)\n",
    "        # get currect step\n",
    "        global_step = len(journal)\n",
    "\n",
    "\n",
    "    # Kill created child process\n",
    "    interpreter.cleanup_session()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ff7c7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
