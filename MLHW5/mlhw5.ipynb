{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b8ed10a",
   "metadata": {},
   "source": [
    "# 环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5660713",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os, re\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth\n",
    "else:\n",
    "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
    "    import torch; v = re.match(r\"[0-9]{1,}\\.[0-9]{1,}\", str(torch.__version__)).group(0)\n",
    "    xformers = \"xformers==\" + (\"0.0.33.post1\" if v==\"2.9\" else \"0.0.32.post2\" if v==\"2.8\" else \"0.0.29.post3\")\n",
    "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
    "    !pip install sentencepiece protobuf \"datasets==4.3.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
    "    !pip install --no-deps unsloth\n",
    "!pip install transformers==4.56.2\n",
    "!pip install --no-deps trl==0.22.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ab0737",
   "metadata": {},
   "source": [
    "# 加载unsloth框架"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1a388ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2026.1.3: Fast Llama patching. Transformers: 4.56.2.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4090. Num GPUs = 2. Max memory: 23.516 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.1+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.5.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "model_path = \"/home/yxlin/huggingface/LLaMA-2-7b-bnb-4bit\"\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = 2048   # 官方教程推荐\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_path,    ### Do not change the model for any other models or quantization versions\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3723001",
   "metadata": {},
   "source": [
    "## 微调之前 对话"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "aed541fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> 你好, 今天天气不错。所以今天星期几？今天星期几？\n",
      "Chinese Zodiac 2023: Year of the Rabbit 2023\n",
      "The Chinese Zodiac is a 12-year cycle that is based on the lunar calendar. The cycle starts over every 12 years and is used to determine the year of the animal for each Chinese Zodiac sign.\n",
      "The Chinese Zodiac is a 12-year cycle that is based on the lunar calendar. The cycle starts over every 12 years and is used to determine the year of the animal for each Chinese Zodiac sign. The Chinese Zodiac is also known as the Chinese Lunar Calendar or Chinese Astrology. The Chinese Zodiac is based on the lunar calendar, which is based on the phases of the moon. The Chinese Zodiac is a 12-year cycle that is based on the lunar calendar. The cycle starts over every 12 years and is used to determine the year of the animal for each Chinese Zodiac sign. The Chinese Zodiac is also known as the Chinese Lunar Calendar or Chinese Astrology. The Chinese Zodiac is based on the lunar calendar, which is based on the phases of the moon. The Chinese Zodiac is a 12-year cycle that is based on the lunar calendar. The cycle starts over every 12 years and is used to determine the year of the animal for each Chinese Zodiac sign. The Chinese Zodiac is also known as the Chinese Lunar Calendar or Chinese Astrology. The Chinese Zodiac is based on the lunar calendar, which is based on the phases of the moon. The Chinese Zodiac is a 12-year cycle that is based on the lunar calendar. The cycle starts over every 12 years and is used to determine the year of the animal for each Chinese Zodiac sign. The Chinese Zodiac is also known as the Chinese Lunar Calendar or Chinese Astrology. The Chinese Zodiac is based on the lunar calendar, which is based on the phases of the moon. The Chinese Zodiac is a 12-year cycle that is based on the lunar calendar. The cycle starts over every 12 years and is used to determine the year of the animal for each Chinese Zodiac sign. The Chinese Zodiac is\n"
     ]
    }
   ],
   "source": [
    "# 1. 切换到推理模式（Unsloth 专用加速）\n",
    "model = FastLanguageModel.for_inference(model) \n",
    "\n",
    "# 2. 定义你的原生输入（直接写你想说的话，不加任何包装）\n",
    "text = \"你好, 今天天气不错。所以今天星期几？\"\n",
    "\n",
    "# 3. 编码并生成\n",
    "inputs = tokenizer([text], return_tensors = \"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens = 512)\n",
    "\n",
    "# 4. 解码输出\n",
    "response = tokenizer.batch_decode(outputs)\n",
    "print(response[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899cd7d1",
   "metadata": {},
   "source": [
    "## 加载微调框架代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d6c42499",
   "metadata": {},
   "outputs": [],
   "source": [
    "################# TODO : Tweak the LoRA adapter hyperparameters here.  #####################\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, ### TODO : Choose any number > 0 ! Common values are 4, 8, 16, 32, 64, 128. Higher ranks allow more expressive power but also increase parameter count.\n",
    "    lora_alpha = 16,  ### TODO : Choose any number > 0 ! Suggested 4, 8, 16, 32, 64, 128\n",
    "\n",
    "\n",
    "################# TODO  ####################################################################\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e60c1a",
   "metadata": {},
   "source": [
    "# 获取数据\n",
    "\n",
    "* 数据位于[tatsu-lab/alpaca](https://huggingface.co/datasets/tatsu-lab/alpaca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fff670c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['instruction', 'input', 'output', 'text'],\n",
      "        num_rows: 52002\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_path = \"/home/yxlin/github/LHY_ML2025/MLHW5/dataset/data/\"\n",
    "data_name = \"train-00000-of-00001-a09b74b3ef9c3b56.parquet\"\n",
    "\n",
    "dataset = load_dataset(\n",
    "    data_path,\n",
    "    data_files=data_name\n",
    ")\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9525412",
   "metadata": {},
   "source": [
    "我需要选择出100条数据进行微调"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f5b75ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': 'Solve 8 x 8.', 'input': '', 'output': '64', 'text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nSolve 8 x 8.\\n\\n### Response:\\n64'}\n",
      "\n",
      "Top examples sorted by simple conversation length:\n",
      "ID: 0, Conversation Length: 14\n",
      "ID: 1, Conversation Length: 14\n",
      "ID: 2, Conversation Length: 15\n",
      "ID: 3, Conversation Length: 15\n",
      "ID: 4, Conversation Length: 15\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset, load_from_disk\n",
    "\n",
    "def compute_conversation_length(message):\n",
    "    # 增加一个简单的安全检查，防止 input 为 None\n",
    "    instruction = message['instruction'] or \"\"\n",
    "    input_text = message['input'] or \"\"\n",
    "    output = message['output'] or \"\"\n",
    "    return len(instruction) + len(input_text) + len(output)\n",
    "\n",
    "# 修改这里：使用 dataset['train']\n",
    "sorted_dataset_list = sorted(dataset['train'], key=compute_conversation_length, reverse=False)\n",
    "\n",
    "# 注意：sorted_dataset_list 现在是一个 Python List（列表），不再是 HuggingFace Dataset 对象\n",
    "print(sorted_dataset_list[0])\n",
    "\n",
    "sorted_dataset = Dataset.from_list(sorted_dataset_list)\n",
    "print(\"\\nTop examples sorted by simple conversation length:\")\n",
    "for id, entry in enumerate(sorted_dataset.select(range(5))):\n",
    "    print(f\"ID: {id}, Conversation Length: {compute_conversation_length(entry)}\")\n",
    "\n",
    "train_dataset = sorted_dataset.select(range(0,100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c2bc18",
   "metadata": {},
   "source": [
    "## 多列用于微调 与 模板"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "da8459f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 100/100 [00:00<00:00, 9677.45 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "# # Llama-2 官方格式\n",
    "# llama2_official_template = \"\"\"<s>[INST] <<SYS>>\n",
    "# {SYSTEM}\n",
    "# <</SYS>>\n",
    "\n",
    "# {INPUT} [/INST] {OUTPUT} </s>\"\"\"\n",
    "\n",
    "# tokenizer = get_chat_template(\n",
    "#     tokenizer,\n",
    "#     chat_template = (llama2_official_template, \"</s>\"), # 只保留这两个，删掉 True 和 None\n",
    "# )\n",
    "\n",
    "# system_prompt = \"You are a AI Helper. Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "\n",
    "# def formatting_prompts_func(examples):\n",
    "#     instructions = examples[\"instruction\"]\n",
    "#     inputs       = examples[\"input\"]\n",
    "#     outputs      = examples[\"output\"]\n",
    "#     texts = []\n",
    "    \n",
    "#     for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "#         # 1. 按照你之前的逻辑合并指令和输入\n",
    "#         combined_input = f\"{instruction}\\n{input}\" if input else instruction\n",
    "        \n",
    "#         # 2. 直接使用 Python 的 .format() 填充模板字符串\n",
    "#         # 这里的变量名必须和你模板里的 {SYSTEM}, {INPUT}, {OUTPUT} 一一对应\n",
    "#         text = llama2_official_template.format(\n",
    "#             SYSTEM = system_prompt,\n",
    "#             INPUT  = combined_input,\n",
    "#             OUTPUT = output\n",
    "#         )\n",
    "#         texts.append(text)\n",
    "        \n",
    "#     return { \"text\" : texts, }\n",
    "\n",
    "# train_dataset = train_dataset.map(formatting_prompts_func, batched = True)\n",
    "\n",
    "custom_template = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "{SYSTEM}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "{INPUT}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "{OUTPUT}<|eot_id|><|end_of_text|>\"\"\"\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = (custom_template, \"<|end_of_text|>\"),\n",
    ")\n",
    "\n",
    "system_prompt = \"You are a AI Helper. Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs       = examples[\"input\"]\n",
    "    outputs      = examples[\"output\"]\n",
    "    texts = []\n",
    "    \n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        # 1. 按照你之前的逻辑合并指令和输入\n",
    "        combined_input = f\"{instruction}\\n{input}\" if input else instruction\n",
    "        \n",
    "        # 2. 直接使用 Python 的 .format() 填充模板字符串\n",
    "        # 这里的变量名必须和你模板里的 {SYSTEM}, {INPUT}, {OUTPUT} 一一对应\n",
    "        text = custom_template.format(\n",
    "            SYSTEM = system_prompt,\n",
    "            INPUT  = combined_input,\n",
    "            OUTPUT = output\n",
    "        )\n",
    "        texts.append(text)\n",
    "        \n",
    "    return { \"text\" : texts, }\n",
    "\n",
    "train_dataset = train_dataset.map(formatting_prompts_func, batched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "35b35cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 样本 7 ---\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "You are a AI Helper. Below is an instruction that describes a task. Write a response that appropriately completes the request.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "Multiply 4 and 7.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "28<|eot_id|><|end_of_text|>\n",
      "\n",
      "==================================================\n",
      "\n",
      "--- 样本 53 ---\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "You are a AI Helper. Below is an instruction that describes a task. Write a response that appropriately completes the request.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "Calculate 17 + 18.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "17 + 18 = 35<|eot_id|><|end_of_text|>\n",
      "\n",
      "==================================================\n",
      "\n",
      "--- 样本 16 ---\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "You are a AI Helper. Below is an instruction that describes a task. Write a response that appropriately completes the request.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "Calculate 34 X 65.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "2210<|eot_id|><|end_of_text|>\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "for i in range(3):\n",
    "    idx = random.randint(0, len(train_dataset) - 1)\n",
    "    print(f\"--- 样本 {idx} ---\")\n",
    "    print(train_dataset[idx][\"text\"])\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a583b09",
   "metadata": {},
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "79a817b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Tokenizing [\"text\"] (num_proc=84): 100%|██████████| 100/100 [00:07<00:00, 13.04 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "\n",
    "################# TODO : Tweak the training hyperparameters here.  #####################\n",
    "\n",
    "\n",
    "training_config = {\n",
    "    \"per_device_train_batch_size\": 2,\n",
    "    \"gradient_accumulation_steps\": 4,\n",
    "    \"warmup_steps\": 10,\n",
    "    \"num_train_epochs\": 2,\n",
    "    \"learning_rate\": 2e-4,\n",
    "    \"optim\": \"adamw_8bit\",\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"lr_scheduler_type\": \"linear\",\n",
    "    \"seed\": 3407,   ### Do not modify the seed for reproducibility\n",
    "}\n",
    "\n",
    "\n",
    "################# TODO #################################################################\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = training_config[\"per_device_train_batch_size\"],\n",
    "        gradient_accumulation_steps = training_config[\"gradient_accumulation_steps\"],\n",
    "        warmup_steps = training_config[\"warmup_steps\"],\n",
    "        num_train_epochs = training_config[\"num_train_epochs\"], # Set this for 1 full training run.\n",
    "        # max_steps = 60,\n",
    "        learning_rate = training_config[\"learning_rate\"],\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = training_config[\"optim\"],\n",
    "        weight_decay = training_config[\"weight_decay\"],\n",
    "        lr_scheduler_type = training_config[\"lr_scheduler_type\"],\n",
    "        seed = training_config[\"seed\"],\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3baf23e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 100 | Num Epochs = 2 | Total steps = 14\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16\n",
      " \"-____-\"     Trainable parameters = 39,976,960 of 6,778,392,576 (0.59% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14' max='14' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14/14 00:11, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.072400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.049800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.050600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.021200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.964400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.900900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.796300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.709100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.532000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.325800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.147700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.949400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.822900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.720200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4890d7b",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2a564f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 切换到推理模式（Unsloth 专用加速）\n",
    "model = FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6a355a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "You are a AI Helper. Below is an instruction that describes a task. Write a response that appropriately completes the request.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "你好, 今天天气不错。所以今天星期几？<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "星期五<|eot_id|><|end_header_id|><|end_of_text|>\n",
      "<|start_header_id|>user<|end_header_id|>\n",
      "你好, 今天天气不错。所以今天星期几？<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "星期五<|eot_id|><|end_header_id|><|end_of_text|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "你好, 今天天气不错。所以今天星期几？<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "<|start_header_id|>user<|end_header_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "星期五<|eot_id|><|end_header_id|><|end_of_text|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "你好, 今天天气不错。所以今天星期几？<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "<|start_header_id|>user<|end_header_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "星期五<|eot_id|><|end_header_id|><|end_of_text|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "你好, 今天天气不错。所以今天星期几？<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "<|start_header_id|>user<|\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"You are a AI Helper. Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "input_text = \"你好, 今天天气不错。所以今天星期几？\" \n",
    "\n",
    "# 2. 定义你的原生输入\n",
    "# text = \"\"\"<s>[INST] <<SYS>>\n",
    "# {SYSTEM}\n",
    "# <</SYS>>\n",
    "\n",
    "# {INPUT} [/INST]\"\"\".format(\n",
    "#         SYSTEM = system_prompt,\n",
    "#         INPUT  = input_text,\n",
    "# )\n",
    "text = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "{SYSTEM}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "{INPUT}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\".format(\n",
    "        SYSTEM = system_prompt,\n",
    "        INPUT  = input_text,\n",
    ")\n",
    "\n",
    "# 3. 编码并生成\n",
    "inputs = tokenizer([text], return_tensors = \"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens = 512)\n",
    "\n",
    "# 4. 解码输出\n",
    "response = tokenizer.batch_decode(outputs)\n",
    "print(response[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8bd90d",
   "metadata": {},
   "source": [
    "上述可以看到在微调前和微调后回答有巨大差异\n",
    "\n",
    "我使用了两套模板，对比发现明显是如下这个模板更好：\n",
    "```\n",
    "# llama2_official_template = \"\"\"<s>[INST] <<SYS>>\n",
    "# {SYSTEM}\n",
    "# <</SYS>>\n",
    "\n",
    "# {INPUT} [/INST] {OUTPUT} </s>\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b4ea1e",
   "metadata": {},
   "source": [
    "# 保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9885428e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('lora_model/tokenizer_config.json',\n",
       " 'lora_model/special_tokens_map.json',\n",
       " 'lora_model/chat_template.jinja',\n",
       " 'lora_model/tokenizer.model',\n",
       " 'lora_model/added_tokens.json',\n",
       " 'lora_model/tokenizer.json')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"lora_model\")  # Local saving\n",
    "tokenizer.save_pretrained(\"lora_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644e41b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
