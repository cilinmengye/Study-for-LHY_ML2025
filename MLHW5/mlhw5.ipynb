{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b8ed10a",
   "metadata": {},
   "source": [
    "# ç¯å¢ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5660713",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os, re\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth\n",
    "else:\n",
    "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
    "    import torch; v = re.match(r\"[0-9]{1,}\\.[0-9]{1,}\", str(torch.__version__)).group(0)\n",
    "    xformers = \"xformers==\" + (\"0.0.33.post1\" if v==\"2.9\" else \"0.0.32.post2\" if v==\"2.8\" else \"0.0.29.post3\")\n",
    "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
    "    !pip install sentencepiece protobuf \"datasets==4.3.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
    "    !pip install --no-deps unsloth\n",
    "!pip install transformers==4.56.2\n",
    "!pip install --no-deps trl==0.22.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ab0737",
   "metadata": {},
   "source": [
    "# åŠ è½½unslothæ¡†æ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a388ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yxlin/github/LHY_ML2025/.venv/lib64/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2026.1.3: Fast Llama patching. Transformers: 4.56.2.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4090. Num GPUs = 2. Max memory: 23.516 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.1+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.5.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "model_path = \"/home/yxlin/huggingface/LLaMA-2-7b-bnb-4bit\"\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = 2048   # å®˜æ–¹æ•™ç¨‹æ¨è\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_path,    ### Do not change the model for any other models or quantization versions\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3723001",
   "metadata": {},
   "source": [
    "## å¾®è°ƒä¹‹å‰ å¯¹è¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aed541fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> ä½ å¥½, ä»Šå¤©å¤©æ°”ä¸é”™ã€‚æ‰€ä»¥ä»Šå¤©æ˜ŸæœŸå‡ ï¼Ÿ\n",
      "The sun is shining brightly today. It is a beautiful day.\n",
      "ä»Šå¤©æ˜¯ä¸€ä¸ªæ¼‚äº®çš„æ—¥å­ï¼Œå¤©å†·å‡‰ï¼Œå¤ªé˜³é—ªçƒç€ï¼Œå¹æ¥äº†ä¸€é˜µæ¸…æ¶¼çš„ä¸œåŒ—é£ï¼Œå¥½äº†ï¼Œä»Šå¤©æ˜ŸæœŸå‡ ï¼Ÿ\n",
      "This is a beautiful day.\n",
      "ä»Šå¤©æ˜¯ä¸€ä¸ªæ¼‚äº®çš„æ—¥å­ã€‚\n",
      "The sun is shining brightly today.\n",
      "å¤©å†·å‡‰ï¼Œå¤ªé˜³é—ªçƒç€ã€‚\n",
      "It is a beautiful day.\n",
      "å¥½äº†ï¼Œä»Šå¤©æ˜ŸæœŸå‡ ï¼Ÿ\n",
      "Today is a beautiful day.\n",
      "ä»Šå¤©æ˜¯ä¸€ä¸ªæ¼‚äº®çš„æ—¥å­ã€‚å¤©å†·å‡‰ï¼Œå¤ªé˜³é—ªçƒç€ã€‚\n",
      "It is a beautiful day. Today is a beautiful day.\n",
      "ä»Šå¤©æ˜ŸæœŸå‡ ï¼Ÿ\n",
      "What is the weather like today?\n",
      "ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ\n",
      "It is a beautiful day. Today is a beautiful day.\n",
      "å¤©å†·å‡‰ï¼Œå¤ªé˜³é—ªçƒç€ï¼Œå¹æ¥äº†ä¸€é˜µæ¸…æ¶¼çš„ä¸œåŒ—é£ï¼Œå¥½äº†ï¼Œä»Šå¤©æ˜ŸæœŸå‡ ï¼Ÿ\n",
      "It is a beautiful day. Today is a beautiful day. å¤©å†·å‡‰ï¼Œå¤ªé˜³é—ªçƒç€ï¼Œå¹æ¥äº†ä¸€é˜µæ¸…æ¶¼çš„ä¸œåŒ—é£ï¼Œå¥½äº†ï¼Œä»Šå¤©æ˜ŸæœŸå‡ ï¼Ÿ\n",
      "It is a beautiful day. Today is a beautiful day. å¤©å†·å‡‰ï¼Œå¤ªé˜³é—ªçƒç€ï¼Œå¹æ¥äº†ä¸€é˜µæ¸…æ¶¼çš„ä¸œåŒ—é£ï¼Œå¥½äº†ï¼Œä»Šå¤©æ˜ŸæœŸå‡ ï¼Ÿ\n",
      "å¤©å†·å‡‰ï¼Œå¤ªé˜³é—ªçƒç€ï¼Œå¹æ¥äº†ä¸€é˜µæ¸…æ¶¼çš„ä¸œåŒ—é£ï¼Œå¥½äº†ï¼Œä»Šå¤©æ˜ŸæœŸå‡ ï¼Ÿ å¤©å†·å‡‰ï¼Œå¤ªé˜³é—ªçƒç€ï¼Œ\n"
     ]
    }
   ],
   "source": [
    "# 1. åˆ‡æ¢åˆ°æ¨ç†æ¨¡å¼ï¼ˆUnsloth ä¸“ç”¨åŠ é€Ÿï¼‰\n",
    "model = FastLanguageModel.for_inference(model) \n",
    "\n",
    "# 2. å®šä¹‰ä½ çš„åŸç”Ÿè¾“å…¥ï¼ˆç›´æ¥å†™ä½ æƒ³è¯´çš„è¯ï¼Œä¸åŠ ä»»ä½•åŒ…è£…ï¼‰\n",
    "text = \"ä½ å¥½, ä»Šå¤©å¤©æ°”ä¸é”™ã€‚æ‰€ä»¥ä»Šå¤©æ˜ŸæœŸå‡ ï¼Ÿ\"\n",
    "\n",
    "# 3. ç¼–ç å¹¶ç”Ÿæˆ\n",
    "inputs = tokenizer([text], return_tensors = \"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens = 512)\n",
    "\n",
    "# 4. è§£ç è¾“å‡º\n",
    "response = tokenizer.batch_decode(outputs)\n",
    "print(response[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a4d1b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.chat_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899cd7d1",
   "metadata": {},
   "source": [
    "## åŠ è½½å¾®è°ƒæ¡†æ¶ä»£ç "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6c42499",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2026.1.3 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "################# TODO : Tweak the LoRA adapter hyperparameters here.  #####################\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, ### TODO : Choose any number > 0 ! Common values are 4, 8, 16, 32, 64, 128. Higher ranks allow more expressive power but also increase parameter count.\n",
    "    lora_alpha = 16,  ### TODO : Choose any number > 0 ! Suggested 4, 8, 16, 32, 64, 128\n",
    "\n",
    "\n",
    "################# TODO  ####################################################################\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e60c1a",
   "metadata": {},
   "source": [
    "# è·å–æ•°æ®\n",
    "\n",
    "* æ•°æ®ä½äº[tatsu-lab/alpaca](https://huggingface.co/datasets/tatsu-lab/alpaca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fff670c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['instruction', 'input', 'output', 'text'],\n",
      "        num_rows: 52002\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_path = \"/home/yxlin/github/LHY_ML2025/MLHW5/dataset/data/\"\n",
    "data_name = \"train-00000-of-00001-a09b74b3ef9c3b56.parquet\"\n",
    "\n",
    "dataset = load_dataset(\n",
    "    data_path,\n",
    "    data_files=data_name\n",
    ")\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9525412",
   "metadata": {},
   "source": [
    "æˆ‘éœ€è¦é€‰æ‹©å‡º100æ¡æ•°æ®è¿›è¡Œå¾®è°ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5b75ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': 'Solve 8 x 8.', 'input': '', 'output': '64', 'text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nSolve 8 x 8.\\n\\n### Response:\\n64'}\n",
      "\n",
      "Top examples sorted by simple conversation length:\n",
      "ID: 0, Conversation Length: 14\n",
      "ID: 1, Conversation Length: 14\n",
      "ID: 2, Conversation Length: 15\n",
      "ID: 3, Conversation Length: 15\n",
      "ID: 4, Conversation Length: 15\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset, load_from_disk\n",
    "\n",
    "def compute_conversation_length(message):\n",
    "    # å¢åŠ ä¸€ä¸ªç®€å•çš„å®‰å…¨æ£€æŸ¥ï¼Œé˜²æ­¢ input ä¸º None\n",
    "    instruction = message['instruction'] or \"\"\n",
    "    input_text = message['input'] or \"\"\n",
    "    output = message['output'] or \"\"\n",
    "    return len(instruction) + len(input_text) + len(output)\n",
    "\n",
    "# ä¿®æ”¹è¿™é‡Œï¼šä½¿ç”¨ dataset['train']\n",
    "sorted_dataset_list = sorted(dataset['train'], key=compute_conversation_length, reverse=False)\n",
    "\n",
    "# æ³¨æ„ï¼šsorted_dataset_list ç°åœ¨æ˜¯ä¸€ä¸ª Python Listï¼ˆåˆ—è¡¨ï¼‰ï¼Œä¸å†æ˜¯ HuggingFace Dataset å¯¹è±¡\n",
    "print(sorted_dataset_list[0])\n",
    "\n",
    "sorted_dataset = Dataset.from_list(sorted_dataset_list)\n",
    "print(\"\\nTop examples sorted by simple conversation length:\")\n",
    "for id, entry in enumerate(sorted_dataset.select(range(5))):\n",
    "    print(f\"ID: {id}, Conversation Length: {compute_conversation_length(entry)}\")\n",
    "\n",
    "train_dataset = sorted_dataset.select(range(0,100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c2bc18",
   "metadata": {},
   "source": [
    "## å¤šåˆ—ç”¨äºå¾®è°ƒ ä¸ æ¨¡æ¿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da8459f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 4393.51 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "# # Llama-2 å®˜æ–¹æ ¼å¼\n",
    "# llama2_official_template = \"\"\"<s>[INST] <<SYS>>\n",
    "# {SYSTEM}\n",
    "# <</SYS>>\n",
    "\n",
    "# {INPUT} [/INST] {OUTPUT} </s>\"\"\"\n",
    "\n",
    "# tokenizer = get_chat_template(\n",
    "#     tokenizer,\n",
    "#     chat_template = (llama2_official_template, \"</s>\"), # åªä¿ç•™è¿™ä¸¤ä¸ªï¼Œåˆ æ‰ True å’Œ None\n",
    "# )\n",
    "\n",
    "# system_prompt = \"You are a AI Helper. Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "\n",
    "# def formatting_prompts_func(examples):\n",
    "#     instructions = examples[\"instruction\"]\n",
    "#     inputs       = examples[\"input\"]\n",
    "#     outputs      = examples[\"output\"]\n",
    "#     texts = []\n",
    "    \n",
    "#     for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "#         # 1. æŒ‰ç…§ä½ ä¹‹å‰çš„é€»è¾‘åˆå¹¶æŒ‡ä»¤å’Œè¾“å…¥\n",
    "#         combined_input = f\"{instruction}\\n{input}\" if input else instruction\n",
    "        \n",
    "#         # 2. ç›´æ¥ä½¿ç”¨ Python çš„ .format() å¡«å……æ¨¡æ¿å­—ç¬¦ä¸²\n",
    "#         # è¿™é‡Œçš„å˜é‡åå¿…é¡»å’Œä½ æ¨¡æ¿é‡Œçš„ {SYSTEM}, {INPUT}, {OUTPUT} ä¸€ä¸€å¯¹åº”\n",
    "#         text = llama2_official_template.format(\n",
    "#             SYSTEM = system_prompt,\n",
    "#             INPUT  = combined_input,\n",
    "#             OUTPUT = output\n",
    "#         )\n",
    "#         texts.append(text)\n",
    "        \n",
    "#     return { \"text\" : texts, }\n",
    "\n",
    "# train_dataset = train_dataset.map(formatting_prompts_func, batched = True)\n",
    "\n",
    "bos_token = \"<s>\"\n",
    "eos_token = \"</s>\"\n",
    "\n",
    "custom_template = \"\"\"{bos_token}<|im_start|>system\n",
    "{SYSTEM}<|im_end|>\n",
    "<|im_start|>user\n",
    "{INPUT}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "{OUTPUT}<|im_end|>{eos_token}\"\"\"\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = (custom_template, eos_token),\n",
    ")\n",
    "\n",
    "system_prompt = \"You are a AI Helper. Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs       = examples[\"input\"]\n",
    "    outputs      = examples[\"output\"]\n",
    "    texts = []\n",
    "    \n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        # 1. æŒ‰ç…§ä½ ä¹‹å‰çš„é€»è¾‘åˆå¹¶æŒ‡ä»¤å’Œè¾“å…¥\n",
    "        combined_input = f\"{instruction}\\n{input}\" if input else instruction\n",
    "        \n",
    "        # 2. ç›´æ¥ä½¿ç”¨ Python çš„ .format() å¡«å……æ¨¡æ¿å­—ç¬¦ä¸²\n",
    "        # è¿™é‡Œçš„å˜é‡åå¿…é¡»å’Œä½ æ¨¡æ¿é‡Œçš„ {SYSTEM}, {INPUT}, {OUTPUT} ä¸€ä¸€å¯¹åº”\n",
    "        text = custom_template.format(\n",
    "            SYSTEM = system_prompt,\n",
    "            INPUT  = combined_input,\n",
    "            OUTPUT = output,\n",
    "            bos_token = bos_token,\n",
    "            eos_token = eos_token,\n",
    "        )\n",
    "        texts.append(text)\n",
    "        \n",
    "    return { \"text\" : texts, }\n",
    "\n",
    "train_dataset = train_dataset.map(formatting_prompts_func, batched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d26d2dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{bos_token}<|im_start|>system\n",
      "{SYSTEM}<|im_end|>\n",
      "<|im_start|>user\n",
      "{INPUT}<|im_end|>\n",
      "<|im_start|>assistant\n",
      "{OUTPUT}<|im_end|>{eos_token}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "35b35cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- æ ·æœ¬ 7 ---\n",
      "<s><|im_start|>system\n",
      "You are a AI Helper. Below is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n",
      "<|im_start|>user\n",
      "Multiply 4 and 7.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "28<|im_end|></s>\n",
      "\n",
      "==================================================\n",
      "\n",
      "--- æ ·æœ¬ 53 ---\n",
      "<s><|im_start|>system\n",
      "You are a AI Helper. Below is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n",
      "<|im_start|>user\n",
      "Calculate 17 + 18.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "17 + 18 = 35<|im_end|></s>\n",
      "\n",
      "==================================================\n",
      "\n",
      "--- æ ·æœ¬ 16 ---\n",
      "<s><|im_start|>system\n",
      "You are a AI Helper. Below is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n",
      "<|im_start|>user\n",
      "Calculate 34 X 65.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "2210<|im_end|></s>\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "for i in range(3):\n",
    "    idx = random.randint(0, len(train_dataset) - 1)\n",
    "    print(f\"--- æ ·æœ¬ {idx} ---\")\n",
    "    print(train_dataset[idx][\"text\"])\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a583b09",
   "metadata": {},
   "source": [
    "# è®­ç»ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "79a817b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Tokenizing [\"text\"] (num_proc=84): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:07<00:00, 12.75 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "\n",
    "################# TODO : Tweak the training hyperparameters here.  #####################\n",
    "\n",
    "\n",
    "training_config = {\n",
    "    \"per_device_train_batch_size\": 2,\n",
    "    \"gradient_accumulation_steps\": 4,\n",
    "    \"warmup_steps\": 10,\n",
    "    \"num_train_epochs\": 2,\n",
    "    \"learning_rate\": 2e-4,\n",
    "    \"optim\": \"adamw_8bit\",\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"lr_scheduler_type\": \"linear\",\n",
    "    \"seed\": 3407,   ### Do not modify the seed for reproducibility\n",
    "}\n",
    "\n",
    "\n",
    "################# TODO #################################################################\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = training_config[\"per_device_train_batch_size\"],\n",
    "        gradient_accumulation_steps = training_config[\"gradient_accumulation_steps\"],\n",
    "        warmup_steps = training_config[\"warmup_steps\"],\n",
    "        num_train_epochs = training_config[\"num_train_epochs\"], # Set this for 1 full training run.\n",
    "        # max_steps = 60,\n",
    "        learning_rate = training_config[\"learning_rate\"],\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = training_config[\"optim\"],\n",
    "        weight_decay = training_config[\"weight_decay\"],\n",
    "        lr_scheduler_type = training_config[\"lr_scheduler_type\"],\n",
    "        seed = training_config[\"seed\"],\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "3baf23e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 100 | Num Epochs = 2 | Total steps = 14\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16\n",
      " \"-____-\"     Trainable parameters = 39,976,960 of 6,778,392,576 (0.59% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14' max='14' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14/14 00:12, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.561300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.518300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.537100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.523100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.490400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.408200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.238800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.031400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.708700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.382800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.162900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.017200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.805000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.668000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4890d7b",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "2a564f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. åˆ‡æ¢åˆ°æ¨ç†æ¨¡å¼ï¼ˆUnsloth ä¸“ç”¨åŠ é€Ÿï¼‰\n",
    "model = FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a355a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s><s> <|im_start|>system\n",
      "You are a AI Helper. Below is an instruction that describes a task. Write a response that appropriately completes the request.<|im_end|>\n",
      "<|im_start|>user\n",
      "ä½ å¥½, ä»Šå¤©å¤©æ°”ä¸é”™ã€‚æ‰€ä»¥ä»Šå¤©æ˜ŸæœŸå‡ ï¼Ÿ<|im_end|>\n",
      "<|im_start|>assistant\n",
      "æ˜ŸæœŸå¤©<|im_end|></s>\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "system_prompt = \"You are a AI Helper. Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "input_text = \"ä½ å¥½, ä»Šå¤©å¤©æ°”ä¸é”™ã€‚æ‰€ä»¥ä»Šå¤©æ˜ŸæœŸå‡ ï¼Ÿ\" \n",
    "\n",
    "# 2. å®šä¹‰ä½ çš„åŸç”Ÿè¾“å…¥\n",
    "# text = \"\"\"<s>[INST] <<SYS>>\n",
    "# {SYSTEM}\n",
    "# <</SYS>>\n",
    "\n",
    "# {INPUT} [/INST]\"\"\".format(\n",
    "#         SYSTEM = system_prompt,\n",
    "#         INPUT  = input_text,\n",
    "# )\n",
    "# text = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "# {SYSTEM}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "# {INPUT}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\".format(\n",
    "#         SYSTEM = system_prompt,\n",
    "#         INPUT  = input_text,\n",
    "# )\n",
    "text = \"\"\"<s><|im_start|>system\n",
    "{SYSTEM}<|im_end|>\n",
    "<|im_start|>user\n",
    "{INPUT}<|im_end|>\n",
    "<|im_start|>assistant\"\"\".format(\n",
    "         SYSTEM = system_prompt,\n",
    "         INPUT  = input_text,\n",
    ")\n",
    "\n",
    "# 3. ç¼–ç å¹¶ç”Ÿæˆ\n",
    "inputs = tokenizer([text], return_tensors = \"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens = 512, eos_token_id = tokenizer.eos_token_id,)\n",
    "\n",
    "# 4. è§£ç è¾“å‡º\n",
    "response = tokenizer.batch_decode(outputs)\n",
    "print(response[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8bd90d",
   "metadata": {},
   "source": [
    "ä¸Šè¿°å¯ä»¥çœ‹åˆ°åœ¨å¾®è°ƒå‰å’Œå¾®è°ƒåå›ç­”æœ‰å·¨å¤§å·®å¼‚\n",
    "\n",
    "æˆ‘ä½¿ç”¨äº†ä¸¤å¥—æ¨¡æ¿ï¼Œå¯¹æ¯”å‘ç°æ˜æ˜¾æ˜¯å¦‚ä¸‹è¿™ä¸ªæ¨¡æ¿æ›´å¥½ï¼š\n",
    "```\n",
    "# llama2_official_template = \"\"\"<s>[INST] <<SYS>>\n",
    "# {SYSTEM}\n",
    "# <</SYS>>\n",
    "\n",
    "# {INPUT} [/INST] {OUTPUT} </s>\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b4ea1e",
   "metadata": {},
   "source": [
    "# ä¿å­˜æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9885428e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('lora_model/tokenizer_config.json',\n",
       " 'lora_model/special_tokens_map.json',\n",
       " 'lora_model/chat_template.jinja',\n",
       " 'lora_model/tokenizer.model',\n",
       " 'lora_model/added_tokens.json',\n",
       " 'lora_model/tokenizer.json')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"lora_model\")  # Local saving\n",
    "tokenizer.save_pretrained(\"lora_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644e41b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
