{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec035c2a",
   "metadata": {},
   "source": [
    "æ­¤ipynbç›¸è¾ƒmlhw5.ipynbæ˜¯ä¸ºäº†éªŒè¯æˆ‘å¯¹chat templateå®ç°è€Œå†™çš„"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694dbd25",
   "metadata": {},
   "source": [
    "# æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cf2ecf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yxlin/github/LHY_ML2025/.venv/lib64/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2026.1.3: Fast Llama patching. Transformers: 4.56.2.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4090. Num GPUs = 2. Max memory: 23.516 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.1+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.5.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2026.1.3 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model_path = \"/home/yxlin/huggingface/LLaMA-2-7b-bnb-4bit\"\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = 2048   # å®˜æ–¹æ•™ç¨‹æ¨è\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_path,    ### Do not change the model for any other models or quantization versions\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, ### TODO : Choose any number > 0 ! Common values are 4, 8, 16, 32, 64, 128. Higher ranks allow more expressive power but also increase parameter count.\n",
    "    lora_alpha = 16,  ### TODO : Choose any number > 0 ! Suggested 4, 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2b0050",
   "metadata": {},
   "source": [
    "# æ•°æ®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "994a4f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['instruction', 'input', 'output', 'text'],\n",
      "        num_rows: 52002\n",
      "    })\n",
      "})\n",
      "{'instruction': 'Solve 8 x 8.', 'input': '', 'output': '64', 'text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nSolve 8 x 8.\\n\\n### Response:\\n64'}\n",
      "\n",
      "Top examples sorted by simple conversation length:\n",
      "ID: 0, Conversation Length: 14\n",
      "ID: 1, Conversation Length: 14\n",
      "ID: 2, Conversation Length: 15\n",
      "ID: 3, Conversation Length: 15\n",
      "ID: 4, Conversation Length: 15\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_path = \"/home/yxlin/github/LHY_ML2025/MLHW5/dataset/data/\"\n",
    "data_name = \"train-00000-of-00001-a09b74b3ef9c3b56.parquet\"\n",
    "\n",
    "dataset = load_dataset(\n",
    "    data_path,\n",
    "    data_files=data_name\n",
    ")\n",
    "\n",
    "print(dataset)\n",
    "\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "def compute_conversation_length(message):\n",
    "    # å¢åŠ ä¸€ä¸ªç®€å•çš„å®‰å…¨æ£€æŸ¥ï¼Œé˜²æ­¢ input ä¸º None\n",
    "    instruction = message['instruction'] or \"\"\n",
    "    input_text = message['input'] or \"\"\n",
    "    output = message['output'] or \"\"\n",
    "    return len(instruction) + len(input_text) + len(output)\n",
    "\n",
    "# ä¿®æ”¹è¿™é‡Œï¼šä½¿ç”¨ dataset['train']\n",
    "sorted_dataset_list = sorted(dataset['train'], key=compute_conversation_length, reverse=False)\n",
    "\n",
    "# æ³¨æ„ï¼šsorted_dataset_list ç°åœ¨æ˜¯ä¸€ä¸ª Python Listï¼ˆåˆ—è¡¨ï¼‰ï¼Œä¸å†æ˜¯ HuggingFace Dataset å¯¹è±¡\n",
    "print(sorted_dataset_list[0])\n",
    "\n",
    "sorted_dataset_list = sorted_dataset_list[:100]\n",
    "print(\"\\nTop examples sorted by simple conversation length:\")\n",
    "for id, entry in enumerate(sorted_dataset_list[:5]):\n",
    "    print(f\"ID: {id}, Conversation Length: {compute_conversation_length(entry)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5513f2",
   "metadata": {},
   "source": [
    "## format æ•°æ®\n",
    "\n",
    "å°†æ•°æ®ç»„ç»‡æˆå¯ä»¥è¢«`tokenizer.apply_chat_template(messages, tokenize=False)`å¤„ç†çš„å½¢å¼\n",
    "æˆ‘ä»¬çš„æ•°æ®ä¸ºmessagesï¼Œå…¶äº¤ç»™`tokenizer.apply_chat_template`å¤„ç†ï¼Œtokenizerä¼šä¾æ®è‡ªèº«è®¾ç½®çš„æ¨¡æ¿ä»messagesä¸­æ‰¾æ•°æ®å¡«è¡¥è¿›è¡Œï¼Œå½¢æˆæœ€ç»ˆå–‚ç»™æ¨¡å‹è®­ç»ƒçš„æ ‡å‡†çš„æ•°æ®\n",
    "\n",
    "messagesæ˜¯ä¸ªlist, å…¶ä¸­å…ƒç´ ä¸ºdict:\n",
    "```\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„æ•°å­¦å®¶ã€‚\"},\n",
    "    {\"role\": \"user\", \"content\": \"1 + 1 ç­‰äºå‡ ï¼Ÿ\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"2\"},\n",
    "    {\"role\": \"system\", \"content\": \"ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„è¯­è¨€å®¶ã€‚\"},\n",
    "    {\"role\": \"user\", \"content\": \"1 + 1 æ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿ\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"æ•°å­¦ä¸Šçš„æ•°å€¼1ä¸¤ä¸¤ç›¸åŠ \"},\n",
    "    ...\n",
    "]\n",
    "```\n",
    "äº¤ç»™`tokenizer.apply_chat_template`å¤„ç†åæˆ‘ä»¬å¯ä»¥å¾—åˆ°å¦‚ä¸‹ç±»ä¼¼è¾“å‡ºï¼Œå–å†³äºtokenizerçš„æ¨¡æ¿ï¼š\n",
    "```\n",
    "<|im_start|>system\\nä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„æ•°å­¦å®¶ã€‚<|im_end|>\\n<|im_start|>user\\n1 + 1 ç­‰äºå‡ ï¼Ÿ<|im_end|>\\n<|im_start|>assistant\\n2<|im_end|>\\n<|im_start|>system\\nä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„è¯­è¨€å®¶ã€‚<|im_end|>\\n<|im_start|>user\\n1 + 1 ç­‰äºå‡ ï¼Ÿ<|im_end|>\\n<|im_start|>assistant\\næ•°å­¦ä¸Šçš„æ•°å€¼1ä¸¤ä¸¤ç›¸åŠ <|im_end|>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2955feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{{- bos_token }}\n",
      "{%- if custom_tools is defined %}\n",
      "    {%- set tools = custom_tools %}\n",
      "{%- endif %}\n",
      "{%- if not tools_in_user_message is defined %}\n",
      "    {%- set tools_in_user_message = true %}\n",
      "{%- endif %}\n",
      "{%- if not date_string is defined %}\n",
      "    {%- set date_string = \"26 July 2024\" %}\n",
      "{%- endif %}\n",
      "{%- if not tools is defined %}\n",
      "    {%- set tools = none %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- This block extracts the system message, so we can slot it into the right place. #}\n",
      "{%- if messages[0]['role'] == 'system' %}\n",
      "    {%- set system_message = messages[0]['content'] %}\n",
      "    {%- set messages = messages[1:] %}\n",
      "{%- else %}\n",
      "    {%- set system_message = \"\" %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- System message + builtin tools #}\n",
      "{{- \"<|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "\" }}\n",
      "{%- if builtin_tools is defined or tools is not none %}\n",
      "    {{- \"Environment: ipython\n",
      "\" }}\n",
      "{%- endif %}\n",
      "{%- if builtin_tools is defined %}\n",
      "    {{- \"Tools: \" + builtin_tools | reject('equalto', 'code_interpreter') | join(\", \") + \"\n",
      "\n",
      "\"}}\n",
      "{%- endif %}\n",
      "{{- \"Cutting Knowledge Date: December 2023\n",
      "\" }}\n",
      "{{- \"Today Date: \" + date_string + \"\n",
      "\n",
      "\" }}\n",
      "{%- if tools is not none and not tools_in_user_message %}\n",
      "    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\n",
      "\n",
      "\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\n",
      "\n",
      "\" }}\n",
      "    {%- endfor %}\n",
      "{%- endif %}\n",
      "{{- system_message }}\n",
      "{{- \"<|eot_id|>\" }}\n",
      "\n",
      "{#- Custom tools are passed in a user message with some extra guidance #}\n",
      "{%- if tools_in_user_message and not tools is none %}\n",
      "    {#- Extract the first user message so we can plug it in here #}\n",
      "    {%- if messages | length != 0 %}\n",
      "        {%- set first_user_message = messages[0]['content'] %}\n",
      "        {%- set messages = messages[1:] %}\n",
      "    {%- else %}\n",
      "        {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n",
      "{%- endif %}\n",
      "    {{- '<|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "' -}}\n",
      "    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n",
      "    {{- \"with its proper arguments that best answers the given prompt.\n",
      "\n",
      "\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\n",
      "\n",
      "\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\n",
      "\n",
      "\" }}\n",
      "    {%- endfor %}\n",
      "    {{- first_user_message + \"<|eot_id|>\"}}\n",
      "{%- endif %}\n",
      "\n",
      "{%- for message in messages %}\n",
      "    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n",
      "        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n",
      "\n",
      "'+ message['content'] + '<|eot_id|>' }}\n",
      "    {%- elif 'tool_calls' in message %}\n",
      "        {%- if not message.tool_calls|length == 1 %}\n",
      "            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n",
      "        {%- endif %}\n",
      "        {%- set tool_call = message.tool_calls[0].function %}\n",
      "        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\n",
      "            {{- '<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "' -}}\n",
      "            {{- \"<|python_tag|>\" + tool_call.name + \".call(\" }}\n",
      "            {%- for arg_name, arg_val in tool_call.arguments | items %}\n",
      "                {{- arg_name + '=\"' + arg_val + '\"' }}\n",
      "                {%- if not loop.last %}\n",
      "                    {{- \", \" }}\n",
      "                {%- endif %}\n",
      "                {%- endfor %}\n",
      "            {{- \")\" }}\n",
      "        {%- else  %}\n",
      "            {{- '<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "' -}}\n",
      "            {{- '{\"name\": \"' + tool_call.name + '\", ' }}\n",
      "            {{- '\"parameters\": ' }}\n",
      "            {{- tool_call.arguments | tojson }}\n",
      "            {{- \"}\" }}\n",
      "        {%- endif %}\n",
      "        {%- if builtin_tools is defined %}\n",
      "            {#- This means we're in ipython mode #}\n",
      "            {{- \"<|eom_id|>\" }}\n",
      "        {%- else %}\n",
      "            {{- \"<|eot_id|>\" }}\n",
      "        {%- endif %}\n",
      "    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n",
      "        {{- \"<|start_header_id|>ipython<|end_header_id|>\n",
      "\n",
      "\" }}\n",
      "        {%- if message.content is mapping or message.content is iterable %}\n",
      "            {{- message.content | tojson }}\n",
      "        {%- else %}\n",
      "            {{- message.content }}\n",
      "        {%- endif %}\n",
      "        {{- \"<|eot_id|>\" }}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "' }}\n",
      "{%- endif %}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.1\",  ### Use llama-3.1 template for better performance here\n",
    ")\n",
    "\n",
    "# æŸ¥çœ‹ç°åœ¨çš„æ¨¡æ¿æ˜¯ä»€ä¹ˆæ ·å­çš„\n",
    "print(tokenizer.chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5ef021d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_message(message: dict, system_prompt: str) -> list[dict]:\n",
    "    '''é’ˆå¯¹ä½¿ç”¨datasetè¯»å–è¡¨æ ¼æ–‡ä»¶åè¿›è¡Œformat'''\n",
    "    chats = []\n",
    "\n",
    "    chats.append(\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"{system_prompt}\"\n",
    "        }\n",
    "    )\n",
    "    chats.append(\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"{message['instruction']}\\n{message['input']}\"\n",
    "        }\n",
    "    )\n",
    "    chats.append(\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": f\"{message['output']}\"\n",
    "        }\n",
    "    )\n",
    "    return chats\n",
    "\n",
    "import re\n",
    "\n",
    "def remove_meta_info(text: str) -> str:\n",
    "    pattern = r\"Cutting Knowledge Date:.*?\\nToday Date:.*?\\n\\n\"\n",
    "    return re.sub(pattern, \"\", text, flags=re.DOTALL)\n",
    "\n",
    "system_prompt = \"You are a AI Helper. Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "train_dataset = []\n",
    "for message in sorted_dataset_list:\n",
    "    chats = format_message(message=message, system_prompt=system_prompt)\n",
    "    train_sample = tokenizer.apply_chat_template(chats, tokenize=False)     # train_sampleæ˜¯ä¸ªstrç±»å‹\n",
    "    train_sample = remove_meta_info(train_sample)   # åˆ é™¤æ‰æ¨¡æ¿ä¸­é™„å¸¦çš„æ—¶é—´ä¿¡æ¯\n",
    "    train_dataset.append(\n",
    "        {\n",
    "            \"text\": train_sample\n",
    "        }\n",
    "    )\n",
    "\n",
    "train_dataset = Dataset.from_list(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f33b75eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- æ ·æœ¬ 42 ---\n",
      "<s><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a AI Helper. Below is an instruction that describes a task. Write a response that appropriately completes the request.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Add 50 to this number.\n",
      "230<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "280<|eot_id|>\n",
      "\n",
      "==================================================\n",
      "\n",
      "--- æ ·æœ¬ 27 ---\n",
      "<s><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a AI Helper. Below is an instruction that describes a task. Write a response that appropriately completes the request.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Divide the number 8 by 2.\n",
      "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "4<|eot_id|>\n",
      "\n",
      "==================================================\n",
      "\n",
      "--- æ ·æœ¬ 51 ---\n",
      "<s><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a AI Helper. Below is an instruction that describes a task. Write a response that appropriately completes the request.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Label the given input.\n",
      "dog<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Noun<|eot_id|>\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "for i in range(3):\n",
    "    idx = random.randint(0, len(train_dataset) - 1)\n",
    "    print(f\"--- æ ·æœ¬ {idx} ---\")\n",
    "    print(train_dataset[idx][\"text\"])\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbed0513",
   "metadata": {},
   "source": [
    "è¿™é‡Œå¾ˆè¯¡å¼‚ï¼Œæ²¡æœ‰ç»“æŸç¬¦</s>ï¼Œè¿™æœ¬æ¥æ˜¯æ¨¡æ¿è´Ÿè´£çš„ï¼Œä½†æ˜¯è¿™ä¹ˆå¹¶æ²¡æœ‰ç”Ÿæˆã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02266afb",
   "metadata": {},
   "source": [
    "# è®­ç»ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "68d2922b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Tokenizing [\"text\"] (num_proc=84): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:06<00:00, 15.90 examples/s]\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 100 | Num Epochs = 2 | Total steps = 14\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16\n",
      " \"-____-\"     Trainable parameters = 39,976,960 of 6,778,392,576 (0.59% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14' max='14' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14/14 00:11, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.187700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.164100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.170700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.152500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.124700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.056700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.900700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.751600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.501400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.256200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.073300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.867900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.750200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.686200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "\n",
    "################# TODO : Tweak the training hyperparameters here.  #####################\n",
    "\n",
    "\n",
    "training_config = {\n",
    "    \"per_device_train_batch_size\": 2,\n",
    "    \"gradient_accumulation_steps\": 4,\n",
    "    \"warmup_steps\": 10,\n",
    "    \"num_train_epochs\": 2,\n",
    "    \"learning_rate\": 2e-4,\n",
    "    \"optim\": \"adamw_8bit\",\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"lr_scheduler_type\": \"linear\",\n",
    "    \"seed\": 3407,   ### Do not modify the seed for reproducibility\n",
    "}\n",
    "\n",
    "\n",
    "################# TODO #################################################################\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = training_config[\"per_device_train_batch_size\"],\n",
    "        gradient_accumulation_steps = training_config[\"gradient_accumulation_steps\"],\n",
    "        warmup_steps = training_config[\"warmup_steps\"],\n",
    "        num_train_epochs = training_config[\"num_train_epochs\"], # Set this for 1 full training run.\n",
    "        # max_steps = 60,\n",
    "        learning_rate = training_config[\"learning_rate\"],\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = training_config[\"optim\"],\n",
    "        weight_decay = training_config[\"weight_decay\"],\n",
    "        lr_scheduler_type = training_config[\"lr_scheduler_type\"],\n",
    "        seed = training_config[\"seed\"],\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")\n",
    "\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485fdc2a",
   "metadata": {},
   "source": [
    "# æ¨ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "63f0c856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. åˆ‡æ¢åˆ°æ¨ç†æ¨¡å¼ï¼ˆUnsloth ä¸“ç”¨åŠ é€Ÿï¼‰\n",
    "model = FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "47c319a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a AI Helper. Below is an instruction that describes a task. Write a response that appropriately completes the request.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "ä½ å¥½, ä»Šå¤©å¤©æ°”ä¸é”™ã€‚æ‰€ä»¥ä»Šå¤©æ˜ŸæœŸå‡ ï¼Ÿ<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"You are a AI Helper. Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "input_text = \"ä½ å¥½, ä»Šå¤©å¤©æ°”ä¸é”™ã€‚æ‰€ä»¥ä»Šå¤©æ˜ŸæœŸå‡ ï¼Ÿ\" \n",
    "text = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": system_prompt,\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": input_text,\n",
    "    }\n",
    "]\n",
    "\n",
    "text = tokenizer.apply_chat_template(text, tokenize=False)     # train_sampleæ˜¯ä¸ªstrç±»å‹\n",
    "text = remove_meta_info(text)   # åˆ é™¤æ‰æ¨¡æ¿ä¸­é™„å¸¦çš„æ—¶é—´ä¿¡æ¯\n",
    "\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4db64db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s><s> <|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a AI Helper. Below is an instruction that describes a task. Write a response that appropriately completes the request.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "ä½ å¥½, ä»Šå¤©å¤©æ°”ä¸é”™ã€‚æ‰€ä»¥ä»Šå¤©æ˜ŸæœŸå‡ ï¼Ÿ<|eot_id|>\n",
      "\n",
      "æ˜ŸæœŸäº”<|eot_id|>\n",
      "\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "æ˜ŸæœŸäº”<|eot_id|>\n",
      "\n",
      "<|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "æ­å–œä½ ï¼Œä½ è¯´çš„å¯¹å§ã€‚<|eot_id|>\n",
      "\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "æ­å–œä½ ï¼Œä½ è¯´çš„å¯¹å§ã€‚<|eot_id|>\n",
      "\n",
      "<|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "æˆ‘å¾ˆé«˜å…´ä½ è¯´çš„å¯¹å§ã€‚<|eot_id|>\n",
      "\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "æˆ‘å¾ˆé«˜å…´ä½ è¯´çš„å¯¹å§ã€‚<|eot_id|>\n",
      "\n",
      "<|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "è°¢è°¢ä½ ã€‚<|eot_id|>\n",
      "\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "ä½ å¥½ã€‚<|eot_id|>\n",
      "\n",
      "<|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "ä½ å¥½ã€‚<|eot_id|>\n",
      "\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "ä½ å¥½ï¼Œæˆ‘æ˜¯æ¨ã€‚<|eot_id|>\n",
      "\n",
      "<|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "ä½ å¥½ï¼Œæˆ‘æ˜¯æ¨ã€‚<|eot_id|>\n",
      "\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "ä½ å¥½ï¼Œæˆ‘æ˜¯æ¨ã€‚<|eot_id|>\n",
      "\n",
      "<|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "ä½ å¥½ï¼Œæˆ‘æ˜¯æ¨ã€‚<|eot_id|>\n",
      "\n",
      "<|start_header_id|>ass\n"
     ]
    }
   ],
   "source": [
    "# 3. ç¼–ç å¹¶ç”Ÿæˆ\n",
    "inputs = tokenizer([text], return_tensors = \"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens = 512)\n",
    "\n",
    "# 4. è§£ç è¾“å‡º\n",
    "response = tokenizer.batch_decode(outputs)\n",
    "print(response[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca025e83",
   "metadata": {},
   "source": [
    "**æˆ‘å¯ä»¥å¦‚ä½•ç†è§£tokenizerçš„chat_template? æˆ‘æœ‰ä¸€ä»½chat_templateï¼Œæˆ‘åº”è¯¥å¦‚ä½•ç†è§£ä»–ï¼Ÿ**\n",
    "\n",
    "ä½ å¯ä»¥æŠŠ `chat_template` ç†è§£ä¸ºä¸€ç§**â€œç¿»è¯‘åè®®â€**ã€‚\n",
    "\n",
    "åœ¨ä¸å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯¹è¯æ—¶ï¼Œæˆ‘ä»¬ä¹ æƒ¯ç”¨â€œç”¨æˆ·â€å’Œâ€œåŠ©æ‰‹â€çš„èº«ä»½è¿›è¡Œäº¤æµã€‚ä½†æ¨¡å‹æœ¬èº«æœ¬è´¨ä¸Šåªæ˜¯ä¸€ä¸ªâ€œæ–‡æœ¬è¡¥å…¨å™¨â€ï¼Œå®ƒå¹¶ä¸çŸ¥é“ä»€ä¹ˆæ˜¯å¯¹è¯ã€‚å®ƒéœ€è¦ç‰¹å®šçš„**ç‰¹æ®Šæ ‡è®°ï¼ˆSpecial Tokensï¼‰**æ¥åŒºåˆ†ï¼šå“ªé‡Œæ˜¯ä½ çš„æŒ‡ä»¤ï¼Œå“ªé‡Œæ˜¯å®ƒçš„å›ç­”ï¼Œå“ªé‡Œåˆæ˜¯ç³»ç»Ÿè®¾å®šã€‚\n",
    "\n",
    "`chat_template` çš„ä½œç”¨å°±æ˜¯ï¼š**å°†ä½ çš„ç»“æ„åŒ–å¯¹è¯å†å²ï¼ˆList of Dictsï¼‰ï¼Œè‡ªåŠ¨è½¬æ¢æˆæ¨¡å‹èƒ½å¬æ‡‚çš„çº¯æ–‡æœ¬å­—ç¬¦ä¸²ã€‚**\n",
    "\n",
    "---\n",
    "\n",
    "### 1. æ ¸å¿ƒé€»è¾‘ï¼šä» JSON åˆ° String\n",
    "\n",
    "é€šå¸¸ä½ çš„å¯¹è¯æ•°æ®é•¿è¿™æ ·ï¼š\n",
    "\n",
    "```python\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„æ•°å­¦å®¶ã€‚\"},\n",
    "    {\"role\": \"user\", \"content\": \"1 + 1 ç­‰äºå‡ ï¼Ÿ\"}\n",
    "]\n",
    "\n",
    "```\n",
    "\n",
    "å¦‚æœæ²¡æœ‰ `chat_template`ï¼Œä½ å¾—æ‰‹åŠ¨æ‹¼æ¥å­—ç¬¦ä¸²ï¼ˆæ¯”å¦‚åŠ  `\\n` æˆ– `User:`ï¼‰ã€‚ä½†ä¸åŒçš„æ¨¡å‹ï¼ˆå¦‚ Llama 3, Qwen, DeepSeekï¼‰è¦æ±‚çš„æ ¼å¼å®Œå…¨ä¸åŒã€‚æœ‰äº†å®ƒï¼Œä½ åªéœ€è¦è°ƒç”¨ï¼š\n",
    "`tokenizer.apply_chat_template(messages, tokenize=False)`\n",
    "\n",
    "å®ƒå°±ä¼šæ ¹æ®æ¨¡æ¿è¾“å‡ºç±»ä¼¼ï¼š\n",
    "`<|im_start|>system\\nä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„æ•°å­¦å®¶ã€‚<|im_end|>\\n<|im_start|>user\\n1 + 1 ç­‰äºå‡ ï¼Ÿ<|im_end|>\\n<|im_start|>assistant\\n`\n",
    "\n",
    "---\n",
    "\n",
    "### 2. å¦‚ä½•è¯»æ‡‚ä¸€ä»½ `chat_template` ä»£ç ï¼Ÿ\n",
    "\n",
    "å¤§éƒ¨åˆ† `chat_template` æ˜¯ç”¨ **Jinja2** è¯­æ³•ç¼–å†™çš„ã€‚ä½ å¯ä»¥å…³æ³¨ä»¥ä¸‹å‡ ä¸ªå…³é”®ç‚¹ï¼š\n",
    "\n",
    "#### A. å¾ªç¯ç»“æ„ (`{% for message in messages %}`)\n",
    "\n",
    "è¿™æ˜¯æ¨¡æ¿çš„æ ¸å¿ƒã€‚å®ƒä¼šéå†ä½ æä¾›çš„å¯¹è¯åˆ—è¡¨ï¼Œé€æ¡å¤„ç†ã€‚\n",
    "\n",
    "#### B. è§’è‰²åˆ¤å®š (`{% if message['role'] == 'user' %}`)\n",
    "\n",
    "æ¨¡æ¿ä¼šæ£€æŸ¥ `role` æ˜¯ `system`ã€`user` è¿˜æ˜¯ `assistant`ï¼Œå¹¶ä¸ºä¸åŒè§’è‰²åŒ…è£¹ä¸åŒçš„**æ§åˆ¶ç¬¦**ã€‚\n",
    "\n",
    "* **System:** é€šå¸¸æ”¾åœ¨å¼€å¤´ï¼Œå®šä¹‰è¾¹ç•Œã€‚\n",
    "* **User:** æç¤ºæ¨¡å‹è¿™æ˜¯è¾“å…¥çš„æŒ‡ä»¤ã€‚\n",
    "* **Assistant:** æç¤ºæ¨¡å‹è¿™æ˜¯å®ƒè¯¥ç”Ÿæˆçš„å›å¤ã€‚\n",
    "\n",
    "#### C. ç‰¹æ®Šæ ‡è®° (BoS/EoS)\n",
    "\n",
    "ä½ ä¼šçœ‹åˆ°ç±»ä¼¼ `{{ bos_token }}` æˆ– `<|im_start|>` è¿™ç§ç¬¦å·ï¼š\n",
    "\n",
    "* **BoS (Beginning of Sequence):** å‘Šè¯‰æ¨¡å‹ï¼Œâ€œå¯¹è¯ä»è¿™é‡Œå¼€å§‹â€ã€‚\n",
    "* **EoS (End of Sequence):** å‘Šè¯‰æ¨¡å‹ï¼Œâ€œè¿™æ¡æ¶ˆæ¯ç»“æŸäº†ï¼Œä½ å¯ä»¥å¼€å§‹å›ç­”æˆ–åœæ­¢äº†â€ã€‚\n",
    "\n",
    "#### D. ç©ºç™½ç¬¦å¤„ç† (`-` ç¬¦å·)\n",
    "\n",
    "å¦‚æœä½ çœ‹åˆ° `{%-` æˆ– `-%}`ï¼Œè¿™ä»£è¡¨**å»æ‰å¤šä½™çš„ç©ºæ ¼æˆ–æ¢è¡Œç¬¦**ï¼Œç¡®ä¿æ‹¼æ¥å‡ºæ¥çš„å­—ç¬¦ä¸²éå¸¸ç´§å‡‘ï¼Œä¸æµªè´¹ Tokenã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### 3. ä¸ºä»€ä¹ˆå®ƒå¦‚æ­¤é‡è¦ï¼Ÿ\n",
    "\n",
    "1. **é˜²æ­¢â€œå¹»è§‰â€å’Œâ€œè§’è‰²é”™ä¹±â€ï¼š** å¦‚æœæ ¼å¼ä¸å¯¹ï¼Œæ¨¡å‹å¯èƒ½ä¼šåˆ†ä¸æ¸…åˆ°åº•æ˜¯è°åœ¨è¯´è¯ï¼Œç”šè‡³ç›´æ¥ç»­å†™ä½ çš„é—®é¢˜è€Œä¸æ˜¯å›ç­”ã€‚\n",
    "2. **è®­ç»ƒä¸æ¨ç†çš„ä¸€è‡´æ€§ï¼š** æ¨¡å‹åœ¨è®­ç»ƒæ—¶ç”¨çš„æ˜¯ä»€ä¹ˆæ ¼å¼ï¼Œæˆ‘ä»¬åœ¨ä½¿ç”¨ï¼ˆæ¨ç†ï¼‰æ—¶å°±å¿…é¡»ä¸¥æ ¼ä¿æŒä¸€è‡´ï¼Œå¦åˆ™æ•ˆæœä¼šå¤§æ‰“æŠ˜æ‰£ã€‚\n",
    "3. **è‡ªåŠ¨æ·»åŠ ç”Ÿæˆå¼•å¯¼ï¼š** å¥½çš„æ¨¡æ¿ä¼šåœ¨æœ€åè‡ªåŠ¨åŠ ä¸Š `assistant\\n` ä¹‹ç±»çš„åç¼€ï¼Œåƒæ¥åŠ›æ£’ä¸€æ ·äº¤ç»™æ¨¡å‹ï¼Œè®©å®ƒçŸ¥é“ï¼šâ€œè¯¥ä½ è¯´è¯äº†â€ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### ä¸€ä¸ªå…¸å‹çš„ä¾‹å­ (ä»¥ Qwen/ChatML ä¸ºä¾‹)\n",
    "\n",
    "```jinja2\n",
    "{% for message in messages %}\n",
    "    {{ '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n' }}\n",
    "{% endfor %}\n",
    "{% if add_generation_prompt %}\n",
    "    {{ '<|im_start|>assistant\\n' }}\n",
    "{% endif %}\n",
    "\n",
    "```\n",
    "\n",
    "* **å‰ä¸‰è¡Œï¼š** æŠŠæ¯æ¡æ¶ˆæ¯åŒ…è£…åœ¨ `<|im_start|>` å’Œ `<|im_end|>` ä¹‹é—´ã€‚\n",
    "* **æœ€åä¸‰è¡Œï¼š** å¦‚æœè®¾ç½®äº† `add_generation_prompt=True`ï¼Œå®ƒä¼šåœ¨æœ«å°¾å¼ºè¡Œå¡å…¥ä¸€ä¸ªåŠ©æ‰‹çš„å¼€å¤´ï¼Œè¯±å¯¼æ¨¡å‹å¼€å§‹ç”Ÿæˆã€‚\n",
    "\n",
    "---\n",
    "\n",
    "**å¦‚æœä½ æ‰‹å¤´æ­£æœ‰ä¸€ä»½å…·ä½“çš„ `chat_template` ä»£ç ï¼Œå¯ä»¥è´´å‡ºæ¥ï¼Œæˆ‘å¯ä»¥å¸®ä½ é€è¡Œè§£æå®ƒçš„å…·ä½“å«ä¹‰ã€‚ä½ æƒ³è®©æˆ‘åˆ†æä¸€ä¸‹å—ï¼Ÿ**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ce2480",
   "metadata": {},
   "source": [
    "**{% å’Œ %}è¿™æ˜¯ä»€ä¹ˆè¯­æ³•ï¼Ÿ**\n",
    "\n",
    "è¿™å±äº **Jinja2** æ¨¡æ¿å¼•æ“çš„è¯­æ³•ã€‚\n",
    "\n",
    "åœ¨ Python é¢†åŸŸï¼ŒJinja2 æ˜¯æœ€æµè¡Œçš„æ¨¡æ¿è¯­è¨€ï¼ˆç±»ä¼¼äº Web å¼€å‘ä¸­å°† HTML å’Œé€»è¾‘ä»£ç ç»“åˆçš„å·¥å…·ï¼‰ã€‚åœ¨ LLMï¼ˆå¤§è¯­è¨€æ¨¡å‹ï¼‰çš„ `tokenizer` ä¸­ï¼Œå®ƒè¢«ä¸“é—¨ç”¨æ¥å¤„ç†å¯¹è¯æ ¼å¼çš„æ‹¼æ¥ã€‚\n",
    "\n",
    "ä½ å¯ä»¥æŠŠ `{% ... %}` ç†è§£ä¸º**â€œé€»è¾‘æ§åˆ¶å—â€**ï¼Œè€ŒæŠŠ `{{ ... }}` ç†è§£ä¸º**â€œå†…å®¹æ‰“å°å—â€**ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### 1. `{% ... %}` ï¼šé€»è¾‘æ§åˆ¶ (Logic)\n",
    "\n",
    "è¿™ç±»ç¬¦å·åŒ…è£¹çš„æ˜¯**æŒ‡ä»¤**ï¼Œå®ƒä¸ç›´æ¥è¾“å‡ºæ–‡æœ¬ï¼Œè€Œæ˜¯å‘Šè¯‰ç¨‹åºè¯¥æ€ä¹ˆåšã€‚\n",
    "\n",
    "* **æ¡ä»¶åˆ¤æ–­ (If/Else):**\n",
    "```jinja2\n",
    "{% if message['role'] == 'user' %}\n",
    "    (å¦‚æœæ˜¯ç”¨æˆ·ï¼Œå°±æ‰§è¡Œè¿™é‡Œçš„æ“ä½œ)\n",
    "{% endif %}\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "* **å¾ªç¯ (For Loop):**\n",
    "```jinja2\n",
    "{% for message in messages %}\n",
    "    (é’ˆå¯¹å¯¹è¯åˆ—è¡¨é‡Œçš„æ¯ä¸€æ¡æ¶ˆæ¯ï¼Œé‡å¤æ‰§è¡Œæ“ä½œ)\n",
    "{% endfor %}\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "* **å˜é‡èµ‹å€¼:**\n",
    "```jinja2\n",
    "{% set loop_messages = messages %}\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 2. `{{ ... }}` ï¼šå†…å®¹è¾“å‡º (Expression)\n",
    "\n",
    "è¿™ç±»ç¬¦å·åŒ…è£¹çš„æ˜¯**å˜é‡æˆ–è¡¨è¾¾å¼**ï¼Œå®ƒä¼šæŠŠç»“æœç›´æ¥â€œå†™â€è¿›æœ€ç»ˆç”Ÿæˆçš„å­—ç¬¦ä¸²é‡Œã€‚\n",
    "\n",
    "* **è¾“å‡ºå˜é‡:** `{{ bos_token }}` ä¼šè¢«æ›¿æ¢æˆå…·ä½“çš„èµ·å§‹ç¬¦ï¼ˆå¦‚ `<s>`ï¼‰ã€‚\n",
    "* **è¾“å‡ºå†…å®¹:** `{{ message['content'] }}` ä¼šè¢«æ›¿æ¢æˆä½ è¾“å…¥çš„å¯¹è¯æ–‡å­—ã€‚\n",
    "* **å­—ç¬¦ä¸²æ‹¼æ¥:** `{{ '<|user|>\\n' + message['content'] }}`ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### 3. ç‰¹æ®Šç¬¦å·ï¼š`{%-` å’Œ `-%}` (å»ç©ºæ ¼)\n",
    "\n",
    "åœ¨ `chat_template` ä¸­ï¼Œä½ ç»å¸¸ä¼šçœ‹åˆ°é‡Œé¢å¤šäº†ä¸€ä¸ªçŸ­æ¨ªçº¿ã€‚è¿™éå¸¸å…³é”®ï¼š\n",
    "\n",
    "* **ä½œç”¨ï¼š** è‡ªåŠ¨å»æ‰è¯¥å—ä¹‹å‰æˆ–ä¹‹åçš„**ç©ºç™½å­—ç¬¦**ï¼ˆç©ºæ ¼ã€åˆ¶è¡¨ç¬¦ã€æ¢è¡Œç¬¦ï¼‰ã€‚\n",
    "* **åŸå› ï¼š** LLM å¯¹ç©ºæ ¼éå¸¸æ•æ„Ÿã€‚å¦‚æœä½ åœ¨ç¼–å†™æ¨¡æ¿æ—¶ä¸ºäº†ç¾è§‚æ¢äº†è¡Œï¼Œä½†ä¸æƒ³è®©è¿™äº›æ¢è¡Œç¬¦è¿›å…¥æœ€ç»ˆçš„ Token ä¸­ï¼Œå°±å¿…é¡»ç”¨ `{%-`ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### æ€»ç»“å¯¹æ¯”\n",
    "\n",
    "| ç¬¦å· | åç§° | ä½œç”¨ | ä¾‹å­ |\n",
    "| --- | --- | --- | --- |\n",
    "| `{% ... %}` | æ§åˆ¶å— | æ‰§è¡Œ**é€»è¾‘**ï¼ˆåˆ¤æ–­ã€å¾ªç¯ï¼‰ | `{% for m in messages %}` |\n",
    "| `{{ ... }}` | å˜é‡å— | æ‰“å°**å†…å®¹**ï¼ˆå˜é‡ã€æ–‡å­—ï¼‰ | `{{ m['content'] }}` |\n",
    "| `{# ... #}` | æ³¨é‡Šå— | å†…éƒ¨ä»£ç **æ³¨é‡Š**ï¼Œä¸è¿è¡Œ | `{# è¿™æ˜¯æ³¨é‡Š #}` |\n",
    "\n",
    "**ç®€å•ç†è§£ï¼š**\n",
    "\n",
    "* `{% %}` æ˜¯**å¤§è„‘**ï¼Œè´Ÿè´£æ€è€ƒé€»è¾‘ï¼ˆå¦‚æœ...é‚£ä¹ˆ...ï¼‰ã€‚\n",
    "* `{{ }}` æ˜¯**å˜´å·´**ï¼Œè´Ÿè´£æŠŠå†…å®¹å¤§å£°è¯´å‡ºæ¥ï¼ˆå¡«å…¥å­—ç¬¦ä¸²ï¼‰ã€‚\n",
    "\n",
    "**ä½ æƒ³è¯•ç€æŠŠä½ çš„é‚£ä»½ `chat_template` è´´å‡ºæ¥ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹å®ƒå…·ä½“ç”¨äº†å“ªäº›é€»è¾‘å—ï¼Ÿ**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4445943e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
